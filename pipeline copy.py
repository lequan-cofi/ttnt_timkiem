import feedparser
import pandas as pd
import numpy as np
import json
import time
import warnings
import sys
import os # ThÃªm thÆ° viá»‡n os
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
# XÃ³a API key cÅ© tá»« biáº¿n mÃ´i trÆ°á»ng náº¿u cÃ³
if 'GOOGLE_API_KEY' in os.environ:
    del os.environ['GOOGLE_API_KEY']

# --- Sá»¬A Lá»–I SUBPROCESS/JOBLIB TRÃŠN WINDOWS ---
# Äáº·t má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh cho sá»‘ lÃµi CPU Ä‘á»ƒ trÃ¡nh lá»—i khi joblib/loky
# cá»‘ gáº¯ng tá»± Ä‘á»™ng Ä‘áº¿m lÃµi trong má»™t sá»‘ mÃ´i trÆ°á»ng phá»©c táº¡p.
os.environ['LOKY_MAX_CPU_COUNT'] = '4'

from bs4 import BeautifulSoup
from dateutil.parser import parse as parse_date
from datetime import datetime, timedelta, timezone
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
from urllib.parse import urlparse
from pyvi import ViTokenizer
from stopwordsiso import stopwords
# --- Cáº¤U HÃŒNH -----------------------------------------------------------------
warnings.filterwarnings('ignore')

# Load environment variables from .env file
load_dotenv()

# Debug print to verify API key and environment
print(f"Current working directory: {os.getcwd()}")
print(f"Environment file path: {os.path.join(os.getcwd(), '.env')}")
print(f"Environment variables: {dict(os.environ)}")

# >>> Sá»¬A Lá»–I UNICODE TRÃŠN WINDOWS CONSOLE <<<
# Buá»™c output cá»§a chÆ°Æ¡ng trÃ¬nh pháº£i lÃ  UTF-8 Ä‘á»ƒ hiá»ƒn thá»‹ tiáº¿ng Viá»‡t chÃ­nh xÃ¡c
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except TypeError: # Bá» qua náº¿u cháº¡y trÃªn mÃ´i trÆ°á»ng khÃ´ng há»— trá»£
    pass


# CÃ¡c nguá»“n RSS Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u
RSS_URLS = [
    "https://dantri.com.vn/rss/home.rss",
    "https://dantri.com.vn/rss/xa-hoi.rss",
    "https://dantri.com.vn/rss/gia-vang.rss",
    "https://dantri.com.vn/rss/the-thao.rss",
    "https://dantri.com.vn/rss/giao-duc.rss",
    "https://dantri.com.vn/rss/kinh-doanh.rss",
    "https://dantri.com.vn/rss/giai-tri.rss",
    "https://dantri.com.vn/rss/phap-luat.rss",
    "https://dantri.com.vn/rss/cong-nghe.rss",
    "https://dantri.com.vn/rss/tinh-yeu-gioi-tinh.rss",
    "https://dantri.com.vn/rss/noi-vu.rss",
    "https://dantri.com.vn/rss/tam-diem.rss",
    "https://dantri.com.vn/rss/infographic.rss",
    "https://dantri.com.vn/rss/dnews.rss",
    "https://dantri.com.vn/rss/xo-so.rss",
    "https://dantri.com.vn/rss/tet-2025.rss",
    "https://dantri.com.vn/rss/d-buzz.rss",
    "https://dantri.com.vn/rss/su-kien.rss",
    "https://dantri.com.vn/rss/the-gioi.rss",
    "https://dantri.com.vn/rss/doi-song.rss",
    "https://dantri.com.vn/rss/lao-dong-viec-lam.rss",
    "https://dantri.com.vn/rss/tam-long-nhan-ai.rss",
    "https://dantri.com.vn/rss/bat-dong-san.rss",
    "https://dantri.com.vn/rss/du-lich.rss",
    "https://dantri.com.vn/rss/suc-khoe.rss",
    "https://dantri.com.vn/rss/o-to-xe-may.rss",
    "https://dantri.com.vn/rss/khoa-hoc.rss",
    "https://dantri.com.vn/rss/ban-doc.rss",
    "https://dantri.com.vn/rss/dmagazine.rss",
    "https://dantri.com.vn/rss/photo-news.rss",
    "https://dantri.com.vn/rss/toa-dam-truc-tuyen.rss",
    "https://dantri.com.vn/rss/interactive.rss",
    "https://dantri.com.vn/rss/photo-story.rss",
    "https://vnexpress.net/rss/tin-moi-nhat.rss",      # Trang chá»§ (thÆ°á»ng lÃ  tin má»›i nháº¥t)
    "https://vnexpress.net/rss/the-gioi.rss",
    "https://vnexpress.net/rss/thoi-su.rss",
    "https://vnexpress.net/rss/kinh-doanh.rss",
    "https://vnexpress.net/rss/startup.rss",
    "https://vnexpress.net/rss/giai-tri.rss",
    "https://vnexpress.net/rss/the-thao.rss",
    "https://vnexpress.net/rss/phap-luat.rss",
    "https://vnexpress.net/rss/giao-duc.rss",
    "https://vnexpress.net/rss/tin-noi-bat.rss",
    
    # Cá»™t bÃªn pháº£i
    "https://vnexpress.net/rss/suc-khoe.rss",
    "https://vnexpress.net/rss/doi-song.rss",
    "https://vnexpress.net/rss/du-lich.rss",
    "https://vnexpress.net/rss/khoa-hoc.rss",         # Khoa há»c cÃ´ng nghá»‡
    "https://vnexpress.net/rss/xe.rss",
    "https://vnexpress.net/rss/y-kien.rss",
    "https://vnexpress.net/rss/tam-su.rss",
    "https://vnexpress.net/rss/cuoi.rss",
    "https://vnexpress.net/rss/tin-xem-nhieu.rss",
    "https://thanhnien.vn/rss/home.rss",                   # Trang chá»§
    "https://thanhnien.vn/rss/thoi-su.rss",
    "https://thanhnien.vn/rss/chinh-tri.rss",
    "https://thanhnien.vn/rss/chao-ngay-moi.rss",
    "https://thanhnien.vn/rss/the-gioi.rss",
    "https://thanhnien.vn/rss/kinh-te.rss",
    "https://thanhnien.vn/rss/doi-song.rss",
    "https://thanhnien.vn/rss/suc-khoe.rss",
    "https://thanhnien.vn/rss/gioi-tre.rss",
    "https://thanhnien.vn/rss/tieu-dung-thong-minh.rss",
    "https://thanhnien.vn/rss/giao-duc.rss",
    "https://thanhnien.vn/rss/du-lich.rss",
    "https://thanhnien.vn/rss/van-hoa.rss",
    "https://thanhnien.vn/rss/giai-tri.rss",
    "https://thanhnien.vn/rss/the-thao.rss",
    "https://thanhnien.vn/rss/cong-nghe.rss",
    "https://thanhnien.vn/rss/xe.rss",
    "https://thanhnien.vn/rss/thoi-trang-tre.rss",
    "https://thanhnien.vn/rss/ban-doc.rss",
    "https://thanhnien.vn/rss/rao-vat.rss",
    "https://thanhnien.vn/rss/video.rss",
    "https://thanhnien.vn/rss/dien-dan.rss",
    "https://thanhnien.vn/rss/podcast.rss",
    "https://thanhnien.vn/rss/nhat-ky-tet-viet.rss",
    "https://thanhnien.vn/rss/magazine.rss",
    "https://thanhnien.vn/rss/cung-con-di-tiep-cuoc-doi.rss",
    "https://thanhnien.vn/rss/ban-can-biet.rss",
    "https://thanhnien.vn/rss/cai-chinh.rss",
    "https://thanhnien.vn/rss/blog-phong-vien.rss",
    "https://thanhnien.vn/rss/toi-viet.rss",
    "https://thanhnien.vn/rss/viec-lam.rss",
    "https://thanhnien.vn/rss/tno.rss",
    "https://thanhnien.vn/rss/tin-24h.rss",
    "https://thanhnien.vn/rss/thi-truong.rss",
    "https://thanhnien.vn/rss/tin-nhanh-360.tno",
     # Cá»™t bÃªn trÃ¡i
    "https://tuoitre.vn/rss/tin-moi-nhat.rss",  # Trang chá»§
    "https://tuoitre.vn/rss/the-gioi.rss",
    "https://tuoitre.vn/rss/kinh-doanh.rss",
    "https://tuoitre.vn/rss/xe.rss",
    "https://tuoitre.vn/rss/van-hoa.rss",
    "https://tuoitre.vn/rss/the-thao.rss",
    "https://tuoitre.vn/rss/khoa-hoc.rss",
    "https://tuoitre.vn/rss/gia-that.rss",
    "https://tuoitre.vn/rss/ban-doc.rss",
    "https://tuoitre.vn/rss/video.rss",

    # Cá»™t bÃªn pháº£i
    "https://tuoitre.vn/rss/thoi-su.rss",
    "https://tuoitre.vn/rss/phap-luat.rss",
    "https://tuoitre.vn/rss/cong-nghe.rss",
    "https://tuoitre.vn/rss/nhip-song-tre.rss",
    "https://tuoitre.vn/rss/giai-tri.rss",
    "https://tuoitre.vn/rss/giao-duc.rss",
    "https://tuoitre.vn/rss/suc-khoe.rss",
    "https://tuoitre.vn/rss/thu-gian.rss",
    "https://tuoitre.vn/rss/du-lich.rss"
]

# Cáº¥u hÃ¬nh cho cÃ¡c mÃ´ hÃ¬nhgit 
NUM_CLUSTERS = 12
SBERT_MODEL = 'Cloyne/vietnamese-sbert-v3'

# --- KIá»‚M TRA VÃ€ Cáº¤U HÃŒNH API KEY (PHáº¦N Gá»  Lá»–I) ----------------------------
print("--- KIá»‚M TRA API KEY ---")
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y API Key trong biáº¿n mÃ´i trÆ°á»ng.")
    print("Vui lÃ²ng táº¡o file .env vÃ  thÃªm dÃ²ng sau:")
    print("GOOGLE_API_KEY=your_api_key_here")
    exit()  # Dá»«ng chÆ°Æ¡ng trÃ¬nh ngay láº­p tá»©c
else:
    # In ra má»™t pháº§n cá»§a key Ä‘á»ƒ xÃ¡c nháº­n
    print(f"âœ… ÄÃ£ tÃ¬m tháº¥y API Key. Báº¯t Ä‘áº§u báº±ng: '{api_key[:4]}...'. Káº¿t thÃºc báº±ng: '...{api_key[-4:]}'.")
    print("Äang cáº¥u hÃ¬nh vá»›i Google...")
    try:
        genai.configure(api_key=api_key)
        print("âœ… Cáº¥u hÃ¬nh Google API thÃ nh cÃ´ng.")
    except Exception as e:
        print(f"âŒ Lá»–I KHI Cáº¤U HÃŒNH: {e}")
        # KhÃ´ng thoÃ¡t, Ä‘á»ƒ hÃ m generate_meaningful_topic_name xá»­ lÃ½ lá»—i vÃ  tiáº¿p tá»¥c
        pass


# --- CÃC HÃ€M CHá»¨C NÄ‚NG (Giá»¯ nguyÃªn) ----------------------------------------

def get_source_name(link):
    """TrÃ­ch xuáº¥t tÃªn miá»n chÃ­nh tá»« URL Ä‘á»ƒ lÃ m tÃªn nguá»“n."""
    try:
        domain = urlparse(link).netloc
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain.split('.')[0].capitalize()
    except Exception:
        # Báº¯t lá»—i cá»¥ thá»ƒ hÆ¡n lÃ  má»™t except rá»—ng
        return "N/A"

def _parse_feed(url):
    """
    HÃ m phá»¥ trá»£: Táº£i vÃ  phÃ¢n tÃ­ch má»™t URL RSS duy nháº¥t.
    HÃ m nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cháº¡y trong má»™t luá»“ng riÃªng.
    """
    try:
        print(f"   - Báº¯t Ä‘áº§u táº£i: {url}")
        feed = feedparser.parse(url)
        print(f"   - HoÃ n táº¥t táº£i: {url}")
        return feed
    except Exception as e:
        print(f"   - Lá»–I khi táº£i {url}: {e}")
        return None

def fetch_recent_articles_optimized(rss_urls, hours=24):
    """
    PhiÃªn báº£n tá»‘i Æ°u: Láº¥y bÃ i viáº¿t tá»« nhiá»u RSS feed Ä‘á»“ng thá»i,
    lá»c bá» cÃ¡c bÃ i cÃ³ tiÃªu Ä‘á» trÃ¹ng láº·p.
    """
    print(f"\n1. Báº¯t Ä‘áº§u láº¥y cÃ¡c bÃ i viáº¿t trong vÃ²ng {hours} giá» qua (cháº¿ Ä‘á»™ tá»‘i Æ°u)...")
    articles = []
    seen_titles = set()
    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours)
    
    # --- Tá»I Æ¯U HÃ“A: Sá»­ dá»¥ng ThreadPoolExecutor Ä‘á»ƒ táº£i nhiá»u feed cÃ¹ng lÃºc ---
    parsed_feeds = []
    with ThreadPoolExecutor(max_workers=10) as executor: # max_workers: sá»‘ luá»“ng cháº¡y song song
        # Táº¡o cÃ¡c tÃ¡c vá»¥ táº£i feed
        future_to_url = {executor.submit(_parse_feed, url): url for url in rss_urls}
        
        # Láº¥y káº¿t quáº£ khi cÃ¡c tÃ¡c vá»¥ hoÃ n thÃ nh
        for future in as_completed(future_to_url):
            feed = future.result()
            if feed: # Chá»‰ xá»­ lÃ½ náº¿u feed Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng
                parsed_feeds.append(feed)

    print("\n2. ÄÃ£ táº£i xong cÃ¡c feed, báº¯t Ä‘áº§u xá»­ lÃ½ vÃ  lá»c bÃ i viáº¿t...")
    # Láº·p qua cÃ¡c feed Ä‘Ã£ Ä‘Æ°á»£c táº£i vá» thÃ nh cÃ´ng
    for feed in parsed_feeds:
        for entry in feed.entries:
            if entry.title in seen_titles:
                continue

            published_time_str = entry.get("published", "")
            if not published_time_str:
                continue

            try:
                parsed_time = parse_date(published_time_str).astimezone(timezone.utc)
                if parsed_time < time_threshold:
                    continue
                    
                # ThÃªm bÃ i viáº¿t vÃ o danh sÃ¡ch vÃ  Ä‘Ã¡nh dáº¥u tiÃªu Ä‘á» Ä‘Ã£ gáº·p
                summary_raw = entry.get("summary", "")
                image_url = None
                if summary_raw:
                    soup = BeautifulSoup(summary_raw, 'html.parser')
                    img_tag = soup.find('img')
                    if img_tag and 'src' in img_tag.attrs:
                        image_url = img_tag['src']
                
                articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "summary_raw": summary_raw,
                    "published_time": parsed_time.isoformat(),
                    "image_url": image_url,
                    "source": get_source_name(entry.link)
                })
                seen_titles.add(entry.title)

            except (ValueError, TypeError):
                continue
                
    print(f"\n-> ÄÃ£ tÃ¬m tháº¥y {len(articles)} bÃ i viáº¿t má»›i (sau khi lá»c trÃ¹ng láº·p).")
    return pd.DataFrame(articles)


def clean_text(df):
    """
    LÃ m sáº¡ch vÃ  xá»­ lÃ½ vÄƒn báº£n tá»« cá»™t 'summary_raw':
    - Chuyá»ƒn chá»¯ thÆ°á»ng, bá» HTML, dáº¥u cÃ¢u.
    - Bá» bÃ i ngáº¯n (<10 tá»«).
    - TÃ¡ch tá»« tiáº¿ng Viá»‡t báº±ng pyvi.
    - Bá» stopword.
    """
    print("\n2. Äang lÃ m sáº¡ch vÄƒn báº£n...")
    

    # B1: LÃ m sáº¡ch vÄƒn báº£n cÆ¡ báº£n
    summary = (df['summary_raw']
               .str.lower()
               .str.replace(r'<.*?>', '', regex=True)
               .str.replace(r'[^\w\s]', '', regex=True))
    df['summary_cleaned'] = summary

    # B2: Bá» NaN, dÃ²ng rá»—ng
    df.dropna(subset=['summary_cleaned'], inplace=True)
    df = df[df['summary_cleaned'].str.strip() != '']

    # B3: Lá»c bÃ i cÃ³ < 10 tá»«
    original_count = len(df)
    df = df[df['summary_cleaned']
                      .str.split()
                      .str.len() >= 10]
    print(f"-> ÄÃ£ lá»c {original_count - len(df)} bÃ i quÃ¡ ngáº¯n.")

    # B4: Tokenize + bá» stopword
    vi_stop = stopwords("vi")
    def remove_stop_pyvi(text):
        tokens = ViTokenizer.tokenize(text).split()
        filtered = [t for t in tokens if t not in vi_stop]
        return " ".join(filtered)

    df['summary_not_stop_word'] = df['summary_cleaned'].apply(remove_stop_pyvi)

    df = df.reset_index(drop=True)
    print("-> HoÃ n táº¥t lÃ m sáº¡ch.")
    return df
def vectorize_text(sentences, model_name):
    """Vector hÃ³a cÃ¢u báº±ng S-BERT."""
    print(f"3/6: Äang vector hÃ³a vÄƒn báº£n báº±ng mÃ´ hÃ¬nh {model_name}...")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(sentences, show_progress_bar=True)
    print("-> Vector hÃ³a hoÃ n táº¥t.")
    return embeddings

def generate_meaningful_topic_name(keywords, sample_titles):
    """Sá»­ dá»¥ng Gemini Ä‘á»ƒ táº¡o tÃªn chá»§ Ä‘á»."""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        prompt = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ biÃªn táº­p bÃ¡o chÃ­. Dá»±a vÃ o cÃ¡c thÃ´ng tin dÆ°á»›i Ä‘Ã¢y, hÃ£y táº¡o ra má»™t tÃªn chá»§ Ä‘á» ngáº¯n gá»n (khÃ´ng quÃ¡ 6 tá»«) báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ tÃ³m táº¯t ná»™i dung chÃ­nh.
        CÃ¡c tá»« khÃ³a chÃ­nh cá»§a chá»§ Ä‘á»: {keywords}
        Má»™t vÃ i tiÃªu Ä‘á» bÃ i viáº¿t vÃ­ dá»¥:
        - {"\n- ".join(sample_titles)}
        TÃªn chá»§ Ä‘á» gá»£i Ã½:"""
        response = model.generate_content(prompt)
        return response.text.strip().replace("*", "")
    except Exception as e:
        # In ra lá»—i cá»¥ thá»ƒ hÆ¡n
        print(f"  - Lá»—i khi gá»i Gemini API: {type(e).__name__} - {e}. Sá»­ dá»¥ng tá»« khÃ³a lÃ m nhÃ£n thay tháº¿.")
        return keywords

def get_topic_labels(df, num_keywords=5):
    """GÃ¡n nhÃ£n chá»§ Ä‘á» cho cÃ¡c cá»¥m."""
    print("4/6: Äang gÃ¡n nhÃ£n chá»§ Ä‘á» cho cÃ¡c cá»¥m...")
    topic_labels = {}
    for i in range(NUM_CLUSTERS):
        cluster_df = df[df['topic_cluster'] == i]
        cluster_texts = cluster_df['summary_cleaned'].tolist()
        if len(cluster_texts) < 3:
            topic_labels[str(i)] = "Chá»§ Ä‘á» nhá» (Ã­t bÃ i viáº¿t)"
            continue
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(cluster_texts)
        avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1
        top_indices = avg_tfidf_scores.argsort()[-num_keywords:][::-1]
        feature_names = vectorizer.get_feature_names_out()
        keywords = ", ".join([feature_names[j] for j in top_indices])
        sample_titles = cluster_df['title'].head(3).tolist()
        meaningful_name = generate_meaningful_topic_name(keywords, sample_titles)
        print(f"  - Cluster {i}: {keywords}  =>  TÃªn chá»§ Ä‘á»: {meaningful_name}")
        topic_labels[str(i)] = meaningful_name
        time.sleep(1)
    print("-> GÃ¡n nhÃ£n chá»§ Ä‘á» hoÃ n táº¥t.")
    return topic_labels

def main_pipeline():
    """HÃ m chÃ­nh cháº¡y toÃ n bá»™ quy trÃ¬nh."""
    print("\nğŸš€ Báº®T Äáº¦U QUY TRÃŒNH Tá»° Äá»˜NG HÃ“A")
    
    # CÃ¡c bÆ°á»›c giá»¯ nguyÃªn
    df = fetch_recent_articles_optimized(RSS_URLS,hours=24)
    if df.empty:
        print("KhÃ´ng cÃ³ bÃ i viáº¿t má»›i nÃ o. Dá»«ng quy trÃ¬nh.")
        return
    df = clean_text(df)
    embeddings = vectorize_text(df['summary_not_stop_word'].tolist(), SBERT_MODEL)
    
    print("Äang thá»±c hiá»‡n phÃ¢n cá»¥m...")
    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)
    df['topic_cluster'] = kmeans.fit_predict(embeddings)
    
    topic_labels = get_topic_labels(df)
    
    print("5/6: Äang tÃ­nh toÃ¡n ma tráº­n tÆ°Æ¡ng Ä‘á»“ng...")
    cosine_sim = cosine_similarity(embeddings)
    
    print("6/6: Äang lÆ°u cÃ¡c file káº¿t quáº£...")
    df.to_csv('final_articles_for_app.csv', index=False, encoding='utf-8-sig')
    np.save('cosine_similarity_matrix.npy', cosine_sim)
    with open('topic_labels.json', 'w', encoding='utf-8') as f:
        json.dump(topic_labels, f, ensure_ascii=False, indent=4)
    
    print("\nâœ… QUY TRÃŒNH HOÃ€N Táº¤T! âœ…")

if __name__ == "__main__":
    main_pipeline()
