# ==== IMPORTS ====
# Khai bÃ¡o cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho pipeline xá»­ lÃ½ dá»¯ liá»‡u.

import feedparser  # ThÆ° viá»‡n chuyÃªn dá»¥ng Ä‘á»ƒ Ä‘á»c vÃ  phÃ¢n tÃ­ch (parse) cÃ¡c nguá»“n cáº¥p dá»¯ liá»‡u RSS/Atom.
import pandas as pd  # DÃ¹ng Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u dáº¡ng báº£ng (DataFrame).
import numpy as np  # DÃ¹ng cho cÃ¡c phÃ©p toÃ¡n sá»‘ há»c, Ä‘áº·c biá»‡t lÃ  vá»›i máº£ng vÃ  ma tráº­n.
import json  # DÃ¹ng Ä‘á»ƒ lÃ m viá»‡c vá»›i dá»¯ liá»‡u Ä‘á»‹nh dáº¡ng JSON (lÆ°u file nhÃ£n chá»§ Ä‘á»).
import time  # Cung cáº¥p cÃ¡c hÃ m liÃªn quan Ä‘áº¿n thá»i gian (vÃ­ dá»¥: time.sleep Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i API).
import warnings  # DÃ¹ng Ä‘á»ƒ quáº£n lÃ½ cÃ¡c cáº£nh bÃ¡o (á»Ÿ Ä‘Ã¢y lÃ  Ä‘á»ƒ bá» qua chÃºng).
import sys  # Cung cáº¥p quyá»n truy cáº­p vÃ o cÃ¡c biáº¿n vÃ  hÃ m cá»§a há»‡ thá»‘ng Python.
import os  # Cung cáº¥p cÃ¡c hÃ m Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i há»‡ Ä‘iá»u hÃ nh (quáº£n lÃ½ file, biáº¿n mÃ´i trÆ°á»ng).
from dotenv import load_dotenv # DÃ¹ng Ä‘á»ƒ táº£i cÃ¡c biáº¿n mÃ´i trÆ°á»ng tá»« file .env, giÃºp quáº£n lÃ½ API key an toÃ n.
import html  # Cung cáº¥p cÃ¡c cÃ´ng cá»¥ Ä‘á»ƒ lÃ m viá»‡c vá»›i cÃ¡c thá»±c thá»ƒ HTML (vÃ­ dá»¥: &amp; -> &).
from sklearn.metrics import silhouette_score # HÃ m tÃ­nh Ä‘iá»ƒm Silhouette Ä‘á»ƒ Ä‘o cháº¥t lÆ°á»£ng cá»§a viá»‡c phÃ¢n cá»¥m.
import numpy as np

# XÃ³a API key cÅ© tá»« biáº¿n mÃ´i trÆ°á»ng náº¿u cÃ³ Ä‘á»ƒ Ä‘áº£m báº£o key tá»« file .env Ä‘Æ°á»£c sá»­ dá»¥ng.
if 'GOOGLE_API_KEY' in os.environ:
    del os.environ['GOOGLE_API_KEY']

# --- Sá»¬A Lá»–I SUBPROCESS/JOBLIB TRÃŠN WINDOWS ---
# Äáº·t má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh cho sá»‘ lÃµi CPU Ä‘á»ƒ trÃ¡nh lá»—i khi joblib/loky
# cá»‘ gáº¯ng tá»± Ä‘á»™ng Ä‘áº¿m lÃµi trong má»™t sá»‘ mÃ´i trÆ°á»ng phá»©c táº¡p (thÆ°á»ng gáº·p khi cháº¡y Ä‘a tiáº¿n trÃ¬nh).
os.environ['LOKY_MAX_CPU_COUNT'] = '4'

from bs4 import BeautifulSoup  # DÃ¹ng Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  trÃ­ch xuáº¥t dá»¯ liá»‡u tá»« file HTML (á»Ÿ Ä‘Ã¢y lÃ  summary tá»« RSS).
from dateutil.parser import parse as parse_date  # ThÆ° viá»‡n máº¡nh máº½ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i nhiá»u Ä‘á»‹nh dáº¡ng chuá»—i thÃ nh Ä‘á»‘i tÆ°á»£ng datetime.
from datetime import datetime, timedelta, timezone  # CÃ¡c lá»›p Ä‘á»ƒ lÃ m viá»‡c vá»›i ngÃ y, giá» vÃ  mÃºi giá».
from sentence_transformers import SentenceTransformer  # ThÆ° viá»‡n chá»©a mÃ´ hÃ¬nh SBERT Ä‘á»ƒ chuyá»ƒn vÄƒn báº£n thÃ nh vector.
from sklearn.cluster import KMeans  # Thuáº­t toÃ¡n phÃ¢n cá»¥m K-Means.
from sklearn.feature_extraction.text import TfidfVectorizer  # DÃ¹ng Ä‘á»ƒ chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh ma tráº­n TF-IDF, há»¯u Ã­ch Ä‘á»ƒ tÃ¬m tá»« khÃ³a.
from sklearn.metrics.pairwise import cosine_similarity  # HÃ m tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng Cosine giá»¯a cÃ¡c vector.
import google.generativeai as genai  # SDK chÃ­nh thá»©c cá»§a Google Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh Gemini.
from urllib.parse import urlparse  # CÃ´ng cá»¥ Ä‘á»ƒ phÃ¢n tÃ­ch má»™t URL thÃ nh cÃ¡c thÃ nh pháº§n cá»§a nÃ³ (tÃªn miá»n, Ä‘Æ°á»ng dáº«n...).
from pyvi import ViTokenizer  # ThÆ° viá»‡n chuyÃªn dá»¥ng Ä‘á»ƒ tÃ¡ch tá»« (tokenize) cho vÄƒn báº£n tiáº¿ng Viá»‡t.
from stopwordsiso import stopwords # ThÆ° viá»‡n cung cáº¥p danh sÃ¡ch cÃ¡c tá»« dá»«ng (stopwords) cho nhiá»u ngÃ´n ngá»¯.

# --- Cáº¤U HÃŒNH -----------------------------------------------------------------
# Bá» qua táº¥t cáº£ cÃ¡c cáº£nh bÃ¡o Ä‘á»ƒ output Ä‘Æ°á»£c sáº¡ch sáº½.
warnings.filterwarnings('ignore')

# Táº£i cÃ¡c biáº¿n mÃ´i trÆ°á»ng tá»« file .env vÃ o os.environ.
# Äiá»u nÃ y cho phÃ©p láº¥y API key mÃ  khÃ´ng cáº§n viáº¿t trá»±c tiáº¿p vÃ o code.
load_dotenv()

# CÃ¡c dÃ²ng debug Ä‘á»ƒ kiá»ƒm tra xem file .env vÃ  API key cÃ³ Ä‘Æ°á»£c táº£i Ä‘Ãºng cÃ¡ch khÃ´ng.
print(f"ThÆ° má»¥c lÃ m viá»‡c hiá»‡n táº¡i: {os.getcwd()}")
print(f"ÄÆ°á»ng dáº«n file .env: {os.path.join(os.getcwd(), '.env')}")

# >>> Sá»¬A Lá»–I UNICODE TRÃŠN WINDOWS CONSOLE <<<
# Buá»™c output cá»§a chÆ°Æ¡ng trÃ¬nh pháº£i lÃ  UTF-8 Ä‘á»ƒ hiá»ƒn thá»‹ tiáº¿ng Viá»‡t chÃ­nh xÃ¡c trÃªn console cá»§a Windows.
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except TypeError:  # Bá» qua náº¿u cháº¡y trÃªn mÃ´i trÆ°á»ng khÃ´ng há»— trá»£ (vÃ­ dá»¥: Linux, macOS Ä‘Ã£ máº·c Ä‘á»‹nh lÃ  UTF-8).
    pass


# Danh sÃ¡ch cÃ¡c nguá»“n cáº¥p dá»¯ liá»‡u RSS sáº½ Ä‘Æ°á»£c thu tháº­p.
# Bao gá»“m nhiá»u chuyÃªn má»¥c tá»« cÃ¡c bÃ¡o lá»›n nhÆ° DÃ¢n TrÃ­, VnExpress, Thanh NiÃªn, Tuá»•i Tráº».
RSS_URLS = [
    # ... (danh sÃ¡ch URL nhÆ° trong code gá»‘c) ...
]

# Cáº¥u hÃ¬nh cho cÃ¡c mÃ´ hÃ¬nh
NUM_CLUSTERS = None  # Sá»‘ cá»¥m sáº½ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tá»± Ä‘á»™ng báº±ng thuáº­t toÃ¡n, khÃ´ng cÃ²n lÃ  giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh.
SBERT_MODEL = 'Cloyne/vietnamese-sbert-v3'  # TÃªn mÃ´ hÃ¬nh S-BERT tiáº¿ng Viá»‡t sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng.

# --- KIá»‚M TRA VÃ€ Cáº¤U HÃŒNH API KEY (PHáº¦N Gá»  Lá»–I) ----------------------------
print("\n--- KIá»‚M TRA API KEY ---")
# Láº¥y giÃ¡ trá»‹ cá»§a biáº¿n mÃ´i trÆ°á»ng 'GOOGLE_API_KEY'.
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    # Náº¿u khÃ´ng tÃ¬m tháº¥y key, in ra hÆ°á»›ng dáº«n vÃ  thoÃ¡t chÆ°Æ¡ng trÃ¬nh.
    print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y API Key trong biáº¿n mÃ´i trÆ°á»ng.")
    print("Vui lÃ²ng táº¡o file .env vÃ  thÃªm dÃ²ng sau:")
    print("GOOGLE_API_KEY=your_api_key_here")
    exit()  # Dá»«ng chÆ°Æ¡ng trÃ¬nh ngay láº­p tá»©c vÃ¬ khÃ´ng thá»ƒ tiáº¿p tá»¥c náº¿u thiáº¿u key.
else:
    # Náº¿u tÃ¬m tháº¥y, in ra má»™t pháº§n cá»§a key Ä‘á»ƒ ngÆ°á»i dÃ¹ng xÃ¡c nháº­n.
    print(f"âœ… ÄÃ£ tÃ¬m tháº¥y API Key. Báº¯t Ä‘áº§u báº±ng: '{api_key[:4]}...'. Káº¿t thÃºc báº±ng: '...{api_key[-4:]}'.")
    print("Äang cáº¥u hÃ¬nh vá»›i Google...")
    try:
        # Cáº¥u hÃ¬nh thÆ° viá»‡n google.generativeai vá»›i API key Ä‘Ã£ láº¥y.
        genai.configure(api_key=api_key)
        print("âœ… Cáº¥u hÃ¬nh Google API thÃ nh cÃ´ng.")
    except Exception as e:
        # Báº¯t lá»—i náº¿u cÃ³ váº¥n Ä‘á» trong quÃ¡ trÃ¬nh cáº¥u hÃ¬nh (vÃ­ dá»¥: key sai Ä‘á»‹nh dáº¡ng).
        print(f"âŒ Lá»–I KHI Cáº¤U HÃŒNH: {e}")
        pass # KhÃ´ng thoÃ¡t, Ä‘á»ƒ cÃ¡c hÃ m sau tá»± xá»­ lÃ½ lá»—i vÃ  cÃ³ thá»ƒ dÃ¹ng phÆ°Æ¡ng Ã¡n dá»± phÃ²ng.


# --- CÃC HÃ€M CHá»¨C NÄ‚NG ----------------------------------------------------

def get_source_name(link):
    """
    Chá»©c nÄƒng: TrÃ­ch xuáº¥t tÃªn miá»n chÃ­nh tá»« URL Ä‘á»ƒ lÃ m tÃªn nguá»“n bÃ¡o.
    Ká»‹ch báº£n: ÄÆ°á»£c gá»i trong lÃºc xá»­ lÃ½ má»—i bÃ i viáº¿t tá»« RSS.
    VÃ­ dá»¥: 'https://vnexpress.net/rss/tin-moi-nhat.rss' -> 'Vnexpress'
    """
    try:
        domain = urlparse(link).netloc  # PhÃ¢n tÃ­ch URL vÃ  láº¥y pháº§n tÃªn miá»n (netloc).
        if domain.startswith('www.'):  # Bá» pháº§n 'www.' náº¿u cÃ³.
            domain = domain[4:]
        return domain.split('.')[0].capitalize()  # TÃ¡ch láº¥y pháº§n Ä‘áº§u tiÃªn vÃ  viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u.
    except:
        return "N/A"  # Tráº£ vá» "N/A" náº¿u cÃ³ lá»—i.


def normalize_title(title):
    """
    Chá»©c nÄƒng: Chuáº©n hÃ³a tiÃªu Ä‘á» Ä‘á»ƒ dá»… dÃ ng so sÃ¡nh vÃ  loáº¡i bá» trÃ¹ng láº·p.
    Logic: Chuyá»ƒn cÃ¡c kÃ½ tá»± HTML (&amp;) thÃ nh kÃ½ tá»± thÆ°á»ng (&), xÃ³a khoáº£ng tráº¯ng thá»«a, vÃ  chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng.
    """
    return html.unescape(title).strip().lower()


def fetch_recent_articles(rss_urls, hours=24):
    """
    Chá»©c nÄƒng: Láº¥y cÃ¡c bÃ i viáº¿t má»›i tá»« danh sÃ¡ch RSS trong má»™t khoáº£ng thá»i gian nháº¥t Ä‘á»‹nh.
    Ká»‹ch báº£n: LÃ  bÆ°á»›c Ä‘áº§u tiÃªn cá»§a pipeline, thu tháº­p dá»¯ liá»‡u thÃ´.
    Logic:
    1. Láº·p qua tá»«ng URL RSS.
    2. PhÃ¢n tÃ­ch RSS feed.
    3. Vá»›i má»—i bÃ i viáº¿t, kiá»ƒm tra xem nÃ³ cÃ³ Ä‘á»§ má»›i khÃ´ng.
    4. Kiá»ƒm tra xem tiÃªu Ä‘á» hoáº·c link Ä‘Ã£ tá»“n táº¡i chÆ°a Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p.
    5. TrÃ­ch xuáº¥t thÃ´ng tin cáº§n thiáº¿t vÃ  thÃªm vÃ o danh sÃ¡ch.
    Tráº£ vá»: Má»™t DataFrame chá»©a cÃ¡c bÃ i viáº¿t Ä‘Ã£ thu tháº­p.
    """
    print(f"\n1. Báº¯t Ä‘áº§u láº¥y cÃ¡c bÃ i viáº¿t trong vÃ²ng {hours} giá» qua...")
    articles = []
    seen_titles = set()  # DÃ¹ng set Ä‘á»ƒ kiá»ƒm tra trÃ¹ng láº·p tiÃªu Ä‘á» hiá»‡u quáº£.
    seen_links = set()   # DÃ¹ng set Ä‘á»ƒ kiá»ƒm tra trÃ¹ng láº·p link hiá»‡u quáº£.
    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours) # Äáº·t má»‘c thá»i gian giá»›i háº¡n.

    for url in rss_urls:
        feed = feedparser.parse(url) # PhÃ¢n tÃ­ch RSS feed.
        for entry in feed.entries:
            norm_title = normalize_title(entry.title) # Chuáº©n hÃ³a tiÃªu Ä‘á».
            link = entry.link.strip() # Láº¥y link vÃ  xÃ³a khoáº£ng tráº¯ng.
            
            # Náº¿u tiÃªu Ä‘á» hoáº·c link Ä‘Ã£ tá»“n táº¡i, bá» qua bÃ i viáº¿t nÃ y.
            if norm_title in seen_titles or link in seen_links:
                continue
            
            published_time = entry.get("published", "") # Láº¥y thá»i gian Ä‘Äƒng bÃ i.
            summary_raw = entry.get("summary", "")     # Láº¥y tÃ³m táº¯t thÃ´ (thÆ°á»ng chá»©a HTML).
            image_url = None
            
            # TrÃ­ch xuáº¥t URL hÃ¬nh áº£nh tá»« trong tháº» <img> cá»§a tÃ³m táº¯t.
            if summary_raw:
                soup = BeautifulSoup(summary_raw, 'html.parser')
                img_tag = soup.find('img')
                if img_tag and 'src' in img_tag.attrs:
                    image_url = img_tag['src']
            
            source_name = get_source_name(entry.link) # Láº¥y tÃªn nguá»“n.
            
            # Xá»­ lÃ½ vÃ  kiá»ƒm tra thá»i gian Ä‘Äƒng bÃ i.
            if published_time:
                try:
                    # Chuyá»ƒn chuá»—i thá»i gian thÃ nh Ä‘á»‘i tÆ°á»£ng datetime cÃ³ nháº­n biáº¿t mÃºi giá» (UTC).
                    parsed_time = parse_date(published_time).astimezone(timezone.utc)
                    # Chá»‰ láº¥y bÃ i má»›i hÆ¡n má»‘c thá»i gian Ä‘Ã£ Ä‘á»‹nh.
                    if parsed_time >= time_threshold:
                        articles.append({
                            "title": entry.title,
                            "link": entry.link,
                            "summary_raw": summary_raw,
                            "published_time": parsed_time.isoformat(), # LÆ°u dÆ°á»›i dáº¡ng chuá»—i chuáº©n ISO.
                            "image_url": image_url,
                            "source": source_name
                        })
                        seen_titles.add(norm_title) # ÄÃ¡nh dáº¥u Ä‘Ã£ tháº¥y.
                        seen_links.add(link)
                except (ValueError, TypeError): # Bá» qua náº¿u Ä‘á»‹nh dáº¡ng thá»i gian khÃ´ng há»£p lá»‡.
                    continue
    print(f"-> ÄÃ£ tÃ¬m tháº¥y {len(articles)} bÃ i viáº¿t má»›i (sau khi lá»c trÃ¹ng láº·p).")
    return pd.DataFrame(articles)


def clean_text(df):
    """
    Chá»©c nÄƒng: LÃ m sáº¡ch vÃ  tiá»n xá»­ lÃ½ vÄƒn báº£n tá»« cá»™t 'summary_raw'.
    Ká»‹ch báº£n: BÆ°á»›c thá»© hai cá»§a pipeline, chuáº©n bá»‹ dá»¯ liá»‡u vÄƒn báº£n cho viá»‡c vector hÃ³a.
    Logic:
    1. Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng, loáº¡i bá» tháº» HTML vÃ  cÃ¡c kÃ½ tá»± khÃ´ng pháº£i chá»¯/sá»‘.
    2. Loáº¡i bá» cÃ¡c bÃ i viáº¿t cÃ³ tÃ³m táº¯t quÃ¡ ngáº¯n (dÆ°á»›i 10 tá»«).
    3. Sá»­ dá»¥ng `pyvi` Ä‘á»ƒ tÃ¡ch tá»« tiáº¿ng Viá»‡t.
    4. Loáº¡i bá» cÃ¡c tá»« dá»«ng (stopwords) trong tiáº¿ng Viá»‡t.
    Tráº£ vá»: DataFrame Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  cÃ³ thÃªm cÃ¡c cá»™t má»›i.
    """
    print("\n2. Äang lÃ m sáº¡ch vÄƒn báº£n...")
    
    # B1: LÃ m sáº¡ch cÆ¡ báº£n báº±ng cÃ¡c hÃ m xá»­ lÃ½ chuá»—i cá»§a pandas.
    summary = (df['summary_raw']
               .str.lower() # Chuyá»ƒn chá»¯ thÆ°á»ng
               .str.replace(r'<.*?>', '', regex=True) # Bá» tháº» HTML
               .str.replace(r'[^\w\s]', '', regex=True)) # Bá» dáº¥u cÃ¢u
    df['summary_cleaned'] = summary

    # B2: Bá» cÃ¡c dÃ²ng cÃ³ tÃ³m táº¯t rá»—ng hoáº·c NaN.
    df.dropna(subset=['summary_cleaned'], inplace=True)
    df = df[df['summary_cleaned'].str.strip() != '']

    # B3: Lá»c cÃ¡c bÃ i viáº¿t cÃ³ Ã­t hÆ¡n 10 tá»« trong tÃ³m táº¯t.
    original_count = len(df)
    df = df[df['summary_cleaned'].str.split().str.len() >= 10]
    print(f"-> ÄÃ£ lá»c {original_count - len(df)} bÃ i quÃ¡ ngáº¯n.")

    # B4: TÃ¡ch tá»« tiáº¿ng Viá»‡t vÃ  loáº¡i bá» tá»« dá»«ng.
    vi_stop = stopwords("vi")
    def remove_stop_pyvi(text):
        tokens = ViTokenizer.tokenize(text).split() # TÃ¡ch tá»«
        filtered = [t for t in tokens if t not in vi_stop] # Lá»c stopwords
        return " ".join(filtered)

    df['summary_not_stop_word'] = df['summary_cleaned'].apply(remove_stop_pyvi)

    df = df.reset_index(drop=True) # Reset láº¡i chá»‰ sá»‘ cá»§a DataFrame.
    print("-> HoÃ n táº¥t lÃ m sáº¡ch.")
    return df


def vectorize_text(sentences, model_name):
    """
    Chá»©c nÄƒng: Chuyá»ƒn Ä‘á»•i má»™t danh sÃ¡ch cÃ¡c cÃ¢u thÃ nh cÃ¡c vector sá»‘ há»c (embeddings).
    Ká»‹ch báº£n: BÆ°á»›c ba cá»§a pipeline, sau khi Ä‘Ã£ cÃ³ vÄƒn báº£n sáº¡ch.
    Sá»­ dá»¥ng: MÃ´ hÃ¬nh Sentence-BERT (SBERT) Ä‘á»ƒ náº¯m báº¯t ngá»¯ nghÄ©a cá»§a cÃ¢u.
    Tráº£ vá»: Má»™t máº£ng NumPy chá»©a cÃ¡c vector embeddings.
    """
    print(f"\n3. Äang vector hÃ³a vÄƒn báº£n báº±ng mÃ´ hÃ¬nh {model_name}...")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(sentences, show_progress_bar=True) # `show_progress_bar=True` Ä‘á»ƒ hiá»ƒn thá»‹ thanh tiáº¿n trÃ¬nh.
    print("-> Vector hÃ³a hoÃ n táº¥t.")
    return embeddings


def find_optimal_clusters(embeddings, max_clusters=20):
    """
    Chá»©c nÄƒng: TÃ¬m sá»‘ cá»¥m (K) tá»‘i Æ°u báº±ng phÆ°Æ¡ng phÃ¡p Silhouette.
    Ká»‹ch báº£n: Cháº¡y trÆ°á»›c khi phÃ¢n cá»¥m chÃ­nh thá»©c Ä‘á»ƒ xÃ¡c Ä‘á»‹nh giÃ¡ trá»‹ K tá»‘t nháº¥t.
    Logic:
    1. Thá»­ cháº¡y K-Means vá»›i cÃ¡c giÃ¡ trá»‹ K khÃ¡c nhau (tá»« 2 Ä‘áº¿n max_clusters).
    2. Vá»›i má»—i giÃ¡ trá»‹ K, tÃ­nh Ä‘iá»ƒm Silhouette. Äiá»ƒm nÃ y Ä‘o lÆ°á»ng má»©c Ä‘á»™ "cháº·t cháº½" vÃ  "tÃ¡ch biá»‡t" cá»§a cÃ¡c cá»¥m.
    3. Chá»n giÃ¡ trá»‹ K cÃ³ Ä‘iá»ƒm Silhouette cao nháº¥t lÃ m K tá»‘i Æ°u.
    Tráº£ vá»: Sá»‘ nguyÃªn lÃ  sá»‘ cá»¥m tá»‘i Æ°u (optimal_k).
    """
    print("\n3.1. Äang tÃ¬m sá»‘ cá»¥m (K) tá»‘i Æ°u báº±ng há»‡ sá»‘ Silhouette...")
    
    silhouette_scores = []
    possible_k_values = range(2, max_clusters + 1)

    for k in possible_k_values:
        # Cháº¡y K-Means vá»›i sá»‘ cá»¥m k. n_init=10 Ä‘á»ƒ cháº¡y láº¡i 10 láº§n vÃ  chá»n káº¿t quáº£ tá»‘t nháº¥t.
        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans_temp.fit_predict(embeddings)
        # TÃ­nh Ä‘iá»ƒm silhouette cho káº¿t quáº£ phÃ¢n cá»¥m nÃ y.
        score = silhouette_score(embeddings, labels)
        silhouette_scores.append(score)
        print(f"   - Vá»›i K={k}, Ä‘iá»ƒm Silhouette = {score:.4f}")

    # TÃ¬m chá»‰ sá»‘ cá»§a Ä‘iá»ƒm cao nháº¥t trong danh sÃ¡ch Ä‘iá»ƒm.
    best_k_index = np.argmax(silhouette_scores)
    # Láº¥y giÃ¡ trá»‹ K tÆ°Æ¡ng á»©ng vá»›i chá»‰ sá»‘ Ä‘Ã³.
    optimal_k = possible_k_values[best_k_index]

    print(f"-> ÄÃ£ tÃ¬m tháº¥y sá»‘ cá»¥m tá»‘i Æ°u lÃ  K={optimal_k} vá»›i Ä‘iá»ƒm Silhouette cao nháº¥t.")
    return optimal_k


def generate_meaningful_topic_name(keywords, sample_titles):
    """
    Chá»©c nÄƒng: Sá»­ dá»¥ng API cá»§a Google Gemini Ä‘á»ƒ táº¡o ra má»™t tÃªn chá»§ Ä‘á» cÃ³ Ã½ nghÄ©a.
    Ká»‹ch báº£n: ÄÆ°á»£c gá»i bá»Ÿi hÃ m get_topic_labels cho má»—i cá»¥m Ä‘Ã£ Ä‘Æ°á»£c táº¡o.
    Logic: Gá»­i má»™t prompt (cÃ¢u lá»‡nh) chi tiáº¿t Ä‘áº¿n Gemini, bao gá»“m cÃ¡c tá»« khÃ³a chÃ­nh vÃ 
           má»™t vÃ i tiÃªu Ä‘á» vÃ­ dá»¥, sau Ä‘Ã³ nháº­n vá» má»™t tÃªn chá»§ Ä‘á» ngáº¯n gá»n.
    Tráº£ vá»: Má»™t chuá»—i lÃ  tÃªn chá»§ Ä‘á».
    """
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Sá»­ dá»¥ng mÃ´ hÃ¬nh Flash nhanh vÃ  tiáº¿t kiá»‡m.
        prompt = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ biÃªn táº­p bÃ¡o chÃ­. Dá»±a vÃ o cÃ¡c thÃ´ng tin dÆ°á»›i Ä‘Ã¢y, hÃ£y táº¡o ra chá»‰ má»™t tÃªn chá»§ Ä‘á» ngáº¯n gá»n duy nháº¥t (khÃ´ng quÃ¡ 6 tá»«) báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ tÃ³m táº¯t ná»™i dung chÃ­nh.
        CÃ¡c tá»« khÃ³a chÃ­nh cá»§a chá»§ Ä‘á»: {keywords}
        Má»™t vÃ i tiÃªu Ä‘á» bÃ i viáº¿t vÃ­ dá»¥:
        - {"\n- ".join(sample_titles)}
        TÃªn chá»§ Ä‘á» gá»£i Ã½:"""
        response = model.generate_content(prompt)
        return response.text.strip().replace("*", "") # LÃ m sáº¡ch output tá»« Gemini.
    except Exception as e:
        # Náº¿u cÃ³ lá»—i khi gá»i API (vÃ­ dá»¥: háº¿t háº¡n má»©c, lá»—i máº¡ng), sáº½ dÃ¹ng chÃ­nh cÃ¡c tá»« khÃ³a lÃ m nhÃ£n.
        print(f"   - Lá»—i khi gá»i Gemini API: {type(e).__name__} - {e}. Sá»­ dá»¥ng tá»« khÃ³a lÃ m nhÃ£n thay tháº¿.")
        return keywords


def get_topic_labels(df, num_keywords=5):
    """
    Chá»©c nÄƒng: Táº¡o nhÃ£n (tÃªn) cÃ³ Ã½ nghÄ©a cho má»—i cá»¥m chá»§ Ä‘á».
    Ká»‹ch báº£n: Cháº¡y sau khi Ä‘Ã£ phÃ¢n cá»¥m xong.
    Logic:
    1. Vá»›i má»—i cá»¥m, trÃ­ch xuáº¥t cÃ¡c bÃ i viáº¿t thuá»™c cá»¥m Ä‘Ã³.
    2. DÃ¹ng TF-IDF Ä‘á»ƒ tÃ¬m ra cÃ¡c tá»« khÃ³a quan trá»ng nháº¥t trong cá»¥m.
    3. Gá»i hÃ m `generate_meaningful_topic_name` Ä‘á»ƒ táº¡o tÃªn chá»§ Ä‘á» tá»« cÃ¡c tá»« khÃ³a vÃ  tiÃªu Ä‘á» máº«u.
    Tráº£ vá»: Má»™t dictionary Ã¡nh xáº¡ ID cá»¥m sang tÃªn chá»§ Ä‘á».
    """
    print("\n5. Äang gÃ¡n nhÃ£n chá»§ Ä‘á» cho cÃ¡c cá»¥m...")
    topic_labels = {}
    actual_clusters = df['topic_cluster'].nunique() # Láº¥y sá»‘ cá»¥m thá»±c táº¿ tá»« dá»¯ liá»‡u.
    
    for i in range(actual_clusters):
        cluster_df = df[df['topic_cluster'] == i]
        cluster_texts = cluster_df['summary_cleaned'].tolist()
        
        # Náº¿u cá»¥m quÃ¡ nhá», khÃ´ng cáº§n phÃ¢n tÃ­ch phá»©c táº¡p.
        if len(cluster_texts) < 3:
            topic_labels[str(i)] = "Chá»§ Ä‘á» nhá» (Ã­t bÃ i viáº¿t)"
            continue
            
        # DÃ¹ng TF-IDF Ä‘á»ƒ tÃ¬m tá»« khÃ³a.
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(cluster_texts)
        avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1 # TÃ­nh Ä‘iá»ƒm TF-IDF trung bÃ¬nh cho má»—i tá»«.
        top_indices = avg_tfidf_scores.argsort()[-num_keywords:][::-1] # Láº¥y chá»‰ sá»‘ cá»§a cÃ¡c tá»« cÃ³ Ä‘iá»ƒm cao nháº¥t.
        feature_names = vectorizer.get_feature_names_out()
        keywords = ", ".join([feature_names[j] for j in top_indices])
        
        # Láº¥y 3 tiÃªu Ä‘á» máº«u.
        sample_titles = cluster_df['title'].head(3).tolist()
        
        # Gá»i Gemini API Ä‘á»ƒ táº¡o tÃªn.
        meaningful_name = generate_meaningful_topic_name(keywords, sample_titles)
        print(f"   - Cluster {i}: {keywords}   =>   TÃªn chá»§ Ä‘á»: {meaningful_name}")
        topic_labels[str(i)] = meaningful_name
        
        # Táº¡m dá»«ng má»™t chÃºt Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ giá»›i háº¡n request cá»§a API (rate limiting).
        time.sleep(1.1)
        
    print("-> GÃ¡n nhÃ£n chá»§ Ä‘á» hoÃ n táº¥t.")
    return topic_labels


def main_pipeline():
    """
    HÃ m chÃ­nh Ä‘iá»u phá»‘i toÃ n bá»™ quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u.
    """
    print("\nğŸš€ Báº®T Äáº¦U QUY TRÃŒNH Xá»¬ LÃ Dá»® LIá»†U ğŸš€")
    
    # BÆ¯á»šC 1: Láº¥y dá»¯ liá»‡u bÃ i viáº¿t má»›i.
    df = fetch_recent_articles(RSS_URLS, hours=24)
    if df.empty:
        print("KhÃ´ng cÃ³ bÃ i viáº¿t má»›i nÃ o. Dá»«ng quy trÃ¬nh.")
        return
        
    # BÆ¯á»šC 2: LÃ m sáº¡ch vÄƒn báº£n.
    df = clean_text(df)
    
    # BÆ¯á»šC 3: Vector hÃ³a vÄƒn báº£n.
    embeddings = vectorize_text(df['summary_not_stop_word'].tolist(), SBERT_MODEL)
    
    # BÆ¯á»šC 3.1: TÃ¬m sá»‘ cá»¥m tá»‘i Æ°u tá»± Ä‘á»™ng.
    NUM_CLUSTERS = find_optimal_clusters(embeddings)
    
    # BÆ¯á»šC 4: Cháº¡y K-Means cuá»‘i cÃ¹ng vá»›i sá»‘ cá»¥m tá»‘i Æ°u Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c.
    print(f"\n4. Äang thá»±c hiá»‡n phÃ¢n cá»¥m K-Means vá»›i {NUM_CLUSTERS} cá»¥m...")
    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)
    df['topic_cluster'] = kmeans.fit_predict(embeddings)
    print("-> PhÃ¢n cá»¥m hoÃ n táº¥t.")

    # In ra thá»‘ng kÃª sá»‘ lÆ°á»£ng bÃ i viáº¿t trong má»—i cá»¥m.
    print("\nğŸ“Š PhÃ¢n bá»• sá»‘ lÆ°á»£ng bÃ i viáº¿t vÃ o cÃ¡c cá»¥m (chá»§ Ä‘á»):")
    cluster_counts = df['topic_cluster'].value_counts().sort_index()
    print(cluster_counts)
    
    # BÆ¯á»šC 5: GÃ¡n nhÃ£n cho cÃ¡c chá»§ Ä‘á».
    topic_labels = get_topic_labels(df)
    
    # BÆ¯á»šC 6: TÃ­nh toÃ¡n ma tráº­n tÆ°Æ¡ng Ä‘á»“ng.
    print("\n6. Äang tÃ­nh toÃ¡n ma tráº­n tÆ°Æ¡ng Ä‘á»“ng...")
    cosine_sim = cosine_similarity(embeddings)
    print("-> TÃ­nh toÃ¡n hoÃ n táº¥t.")
    
    # BÆ¯á»šC 7: LÆ°u táº¥t cáº£ cÃ¡c káº¿t quáº£ ra file Ä‘á»ƒ á»©ng dá»¥ng Streamlit sá»­ dá»¥ng.
    print("\n7. Äang lÆ°u cÃ¡c file káº¿t quáº£...")
    df.to_csv('final_articles_for_app.csv', index=False, encoding='utf-8-sig') # DÃ¹ng 'utf-8-sig' Ä‘á»ƒ Excel Ä‘á»c Ä‘Ãºng tiáº¿ng Viá»‡t.
    np.save('cosine_similarity_matrix.npy', cosine_sim)
    np.save('embeddings.npy', embeddings)
    with open('topic_labels.json', 'w', encoding='utf-8') as f:
        json.dump(topic_labels, f, ensure_ascii=False, indent=4) # `ensure_ascii=False` Ä‘á»ƒ lÆ°u Ä‘Ãºng tiáº¿ng Viá»‡t.
    
    print("\nâœ… QUY TRÃŒNH HOÃ€N Táº¤T! âœ…")


# Äiá»ƒm báº¯t Ä‘áº§u cá»§a chÆ°Æ¡ng trÃ¬nh khi cháº¡y file nÃ y trá»±c tiáº¿p.
if __name__ == "__main__":
    main_pipeline()