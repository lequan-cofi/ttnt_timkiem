# ==== IMPORTS ====
import streamlit as st  # Framework ƒë·ªÉ x√¢y d·ª±ng giao di·ªán web
import pandas as pd  # Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng
import numpy as np  # Th∆∞ vi·ªán t√≠nh to√°n s·ªë h·ªçc
import json  # X·ª≠ l√Ω d·ªØ li·ªáu JSON
import re  # X·ª≠ l√Ω chu·ªói v·ªõi regular expression
import sys  # Truy c·∫≠p c√°c bi·∫øn v√† h√†m c·ªßa h·ªá th·ªëng
import subprocess  # Ch·∫°y c√°c l·ªánh h·ªá th·ªëng
import requests  # G·ª≠i HTTP requests
from bs4 import BeautifulSoup  # Parse v√† x·ª≠ l√Ω HTML
from urllib.parse import urlparse, quote, unquote  # X·ª≠ l√Ω URL
from sklearn.metrics.pairwise import cosine_similarity  # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine
from sentence_transformers import SentenceTransformer  # M√¥ h√¨nh ng√¥n ng·ªØ ƒë·ªÉ t·∫°o vector
# from streamlit_extras.app_logo import app_logo
# --- C·∫§U H√åNH TRANG V√Ä CSS ---

st.set_page_config(page_title="T·∫°p ch√≠ c·ªßa b·∫°n", page_icon="üìñ", layout="wide")

# ==== KH·ªûI T·∫†O TR·∫†NG TH√ÅI ====
# Kh·ªüi t·∫°o c√°c bi·∫øn session state ƒë·ªÉ l∆∞u tr·ªØ tr·∫°ng th√°i
if 'read_articles' not in st.session_state:
    st.session_state.read_articles = set()  # T·∫≠p h·ª£p c√°c b√†i vi·∫øt ƒë√£ ƒë·ªçc
if 'reading_history' not in st.session_state:
    st.session_state.reading_history = []  # L·ªãch s·ª≠ ƒë·ªçc b√†i vi·∫øt
if 'current_view' not in st.session_state:
    st.session_state.current_view = "main"  # View hi·ªán t·∫°i (main/detail)
if 'current_article_id' not in st.session_state:
    st.session_state.current_article_id = None  # ID b√†i vi·∫øt ƒëang xem
if 'selected_topic' not in st.session_state:
    st.session_state.selected_topic = "D√†nh cho b·∫°n (T·∫•t c·∫£)"  # Ch·ªß ƒë·ªÅ ƒë√£ ch·ªçn
if 'selected_sources' not in st.session_state:
    st.session_state.selected_sources = []  # Ngu·ªìn tin ƒë√£ ch·ªçn

@st.cache_resource
def get_sbert_model():
    """L·∫•y m√¥ h√¨nh SBERT ƒë√£ ƒë∆∞·ª£c cache."""
    return SentenceTransformer('Cloyne/vietnamese-sbert-v3')

def local_css(file_name):
    """ƒê·ªçc v√† √°p d·ª•ng CSS t·ª´ file."""
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    except FileNotFoundError:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{file_name}'.")

# ==== H√ÄM T·∫¢I D·ªÆ LI·ªÜU ---
@st.cache_data
def load_data_and_embeddings():
    """T·∫£i d·ªØ li·ªáu v√† embeddings t·ª´ c√°c file."""
    # ƒê·ªçc d·ªØ li·ªáu t·ª´ CSV
    df = pd.read_csv('final_articles_for_app.csv')
    # ƒê·ªçc ma tr·∫≠n ƒë·ªô t∆∞∆°ng ƒë·ªìng
    cosine_sim = np.load('cosine_similarity_matrix.npy')
    # ƒê·ªçc nh√£n ch·ªß ƒë·ªÅ
    with open('topic_labels.json', 'r', encoding='utf-8') as f:
        topic_labels = json.load(f)
    # Chuy·ªÉn ƒë·ªïi th·ªùi gian v·ªõi x·ª≠ l√Ω l·ªói
    try:
        df['published_time'] = pd.to_datetime(df['published_time'], errors='coerce')
        # Lo·∫°i b·ªè c√°c b√†i vi·∫øt c√≥ th·ªùi gian kh√¥ng h·ª£p l·ªá
        df = df.dropna(subset=['published_time'])
    except Exception as e:
        st.error(f"L·ªói khi chuy·ªÉn ƒë·ªïi th·ªùi gian: {e}")
        # N·∫øu l·ªói, gi·ªØ nguy√™n d·∫°ng string
        pass
    # S·ª≠ d·ª•ng c·ªôt source tr·ª±c ti·∫øp t·ª´ CSV
    df['source_name'] = df['source']
    # ƒê·ªçc embeddings
    try:
        embeddings = np.load('embeddings.npy')
    except:
        st.error("Kh√¥ng t√¨m th·∫•y file embeddings.npy. Vui l√≤ng ch·∫°y l·∫°i pipeline ƒë·ªÉ t·∫°o embeddings.")
        st.stop()
    return df, cosine_sim, topic_labels, embeddings

def calculate_average_vector(article_ids, cosine_sim):
    """T√≠nh vector trung b√¨nh t·ª´ 5 b√†i vi·∫øt g·∫ßn nh·∫•t."""
    if not article_ids:
        return None
    
    vectors = []
    for article_id in article_ids:
        if article_id < len(cosine_sim):
            vectors.append(cosine_sim[article_id])
    
    if not vectors:
        return None
    
    return np.mean(vectors, axis=0)

def get_similar_articles_by_history(df, cosine_sim, history_articles, exclude_articles=None):
    """L·∫•y b√†i vi·∫øt t∆∞∆°ng t·ª± d·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc."""
    if not history_articles:
        return pd.DataFrame()
    
    # L·∫•y 5 b√†i vi·∫øt m·ªõi ƒë·ªçc g·∫ßn nh·∫•t
    recent_articles = history_articles[:5]
    
    # T√≠nh vector trung b√¨nh
    avg_vector = calculate_average_vector(recent_articles, cosine_sim)
    if avg_vector is None:
        return pd.DataFrame()
    
    # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng
    similarity_scores = cosine_similarity([avg_vector], cosine_sim)[0]
    
    # Lo·∫°i b·ªè b√†i ƒë√£ ƒë·ªçc
    if exclude_articles:
        mask = ~df.index.isin(exclude_articles)
        similarity_scores[~mask] = -1
    
    # L·∫•y top b√†i vi·∫øt t∆∞∆°ng t·ª±
    top_indices = np.argsort(similarity_scores)[::-1][:10]
    similar_articles = df.iloc[top_indices].copy()
    similar_articles['similarity_score'] = similarity_scores[top_indices]
    
    return similar_articles

def crawl_article_content(url):
    """Crawl n·ªôi dung b√†i vi·∫øt t·ª´ URL v√† l√†m s·∫°ch HTML.
    
    H√†m n√†y th·ª±c hi·ªán c√°c nhi·ªám v·ª•:
    1. T·∫£i n·ªôi dung trang web t·ª´ URL ƒë∆∞·ª£c cung c·∫•p
    2. Ph√¢n t√≠ch HTML ƒë·ªÉ t√¨m n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt
    3. L√†m s·∫°ch HTML b·∫±ng c√°ch lo·∫°i b·ªè c√°c th·∫ª v√† thu·ªôc t√≠nh kh√¥ng c·∫ßn thi·∫øt
    4. X·ª≠ l√Ω c√°c th·∫ª ƒë·∫∑c bi·ªát nh∆∞ ·∫£nh v√† li√™n k·∫øt
    5. Tr·∫£ v·ªÅ n·ªôi dung ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch d∆∞·ªõi d·∫°ng chu·ªói HTML
    """

    try:
        # Thi·∫øt l·∫≠p headers ƒë·ªÉ gi·∫£ l·∫≠p tr√¨nh duy·ªát th·∫≠t
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive'
        }
        
        # T·∫£i n·ªôi dung trang web v·ªõi timeout 10 gi√¢y
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'  # ƒê·∫£m b·∫£o encoding ti·∫øng Vi·ªát
        soup = BeautifulSoup(response.text, 'html.parser')  # Parse HTML

        # ==== X·ª¨ L√ù ·∫¢NH ====
        # Chuy·ªÉn ƒë·ªïi ·∫£nh lazy-load sang ·∫£nh th∆∞·ªùng
        for img in soup.find_all('img'):
            # N·∫øu ·∫£nh c√≥ thu·ªôc t√≠nh data-src (lazy-load), chuy·ªÉn th√†nh src
            if img.has_attr('data-src'):
                img['src'] = img['data-src']
            # X√≥a ·∫£nh kh√¥ng c√≥ src ho·∫∑c src r·ªóng
            if not img.has_attr('src') or not img['src'].strip():
                img.decompose()
            # Th√™m thu·ªôc t√≠nh lazy-load cho t·∫•t c·∫£ ·∫£nh
            img['loading'] = 'lazy'

        # X√≥a c√°c th·∫ª figure kh√¥ng ch·ª©a n·ªôi dung
        for fig in soup.find_all('figure'):
            if not fig.get_text(strip=True) and not fig.find('img'):
                fig.decompose()

        article = None
        
        # ==== X·ª¨ L√ù THEO T·ª™NG TRANG B√ÅO ====
        # VnExpress
        if 'vnexpress.net' in url:
            # T√¨m n·ªôi dung ch√≠nh theo th·ª© t·ª± ∆∞u ti√™n c√°c class
            article = soup.find('article', class_='fck_detail')
            if not article:
                # N·∫øu kh√¥ng t√¨m th·∫•y, l·∫•y article ƒë·∫ßu ti√™n
                article = soup.find('article')
            if article:
                # X√≥a c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
                for tag in article.find_all(['script', 'style', 'iframe']):
                    tag.decompose()
                return str(article)

        # Tu·ªïi Tr·∫ª v√† Thanh Ni√™n
        elif 'tuoitre.vn' in url or 'thanhnien.vn' in url:
            # T√¨m n·ªôi dung theo class detail-content
            article = soup.find('div', class_='detail-content')
            if not article:
                # N·∫øu kh√¥ng t√¨m th·∫•y, l·∫•y div c√≥ nhi·ªÅu text nh·∫•t
                divs = soup.find_all('div')
                article = max(divs, key=lambda d: len(d.get_text(strip=True))) if divs else None

        # D√¢n Tr√≠
        elif 'dantri.com.vn' in url:
            # T√¨m n·ªôi dung theo class dt-news__content
            article = soup.find('div', class_='dt-news__content')
            if not article:
                article = soup.find('article')
            if not article:
                # N·∫øu kh√¥ng t√¨m th·∫•y, l·∫•y div c√≥ nhi·ªÅu text nh·∫•t
                divs = soup.find_all('div')
                article = max(divs, key=lambda d: len(d.get_text(strip=True))) if divs else None

        # ==== L√ÄM S·∫†CH N·ªòI DUNG ====
        if article:
            # 1. X√≥a c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
            for tag in article.find_all(['script', 'style', 'iframe', 'button', 'ins', 'noscript', 'form']):
                tag.decompose()

            # 2. X·ª≠ l√Ω c√°c th·∫ª <a> (li√™n k·∫øt)
            for a in article.find_all('a'):
                # M·ªü link trong tab m·ªõi
                a['target'] = '_blank'
                # Th√™m thu·ªôc t√≠nh b·∫£o m·∫≠t
                a['rel'] = 'noopener noreferrer'
                # X√≥a c√°c thu·ªôc t√≠nh JavaScript
                if a.has_attr('onclick'):
                    del a['onclick']
                # X√≥a c√°c thu·ªôc t√≠nh kh√¥ng c·∫ßn thi·∫øt
                for attr in ['data-', 'on', 'class', 'id']:
                    for a_attr in list(a.attrs):
                        if a_attr.startswith(attr):
                            del a[a_attr]

            # 3. X·ª≠ l√Ω c√°c th·∫ª <img> (·∫£nh)
            for img in article.find_all('img'):
                # Th√™m lazy-load
                img['loading'] = 'lazy'
                # Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n ·∫£nh
                if img.has_attr('src') and not img['src'].startswith(('http://', 'https://')):
                    img['src'] = 'https://vnexpress.net' + img['src']
                # X√≥a c√°c thu·ªôc t√≠nh kh√¥ng c·∫ßn thi·∫øt
                for attr in ['data-', 'on', 'class', 'id']:
                    for img_attr in list(img.attrs):
                        if img_attr.startswith(attr):
                            del img[img_attr]

            # 4. X√≥a c√°c div r·ªóng
            for div in article.find_all('div'):
                if not div.get_text(strip=True) and not div.find('img'):
                    div.decompose()

            # 5. X√≥a m·ªçi thu·ªôc t√≠nh kh√¥ng c·∫ßn thi·∫øt
            for tag in article.find_all(True):
                # Ch·ªâ gi·ªØ l·∫°i c√°c thu·ªôc t√≠nh c·∫ßn thi·∫øt
                allowed_attrs = ['src', 'alt', 'href', 'target', 'rel', 'loading']
                tag.attrs = {k: v for k, v in tag.attrs.items() if k in allowed_attrs}

            return str(article)

        return None

    except Exception as e:
        # In l·ªói ƒë·ªÉ debug
        print(f"L·ªói khi crawl b√†i vi·∫øt t·ª´ {url}: {str(e)}")
        return None

def render_detail_view(article_id, df, cosine_sim, topic_labels):
    """Hi·ªÉn th·ªã chi ti·∫øt b√†i vi·∫øt v√† c√°c b√†i vi·∫øt li√™n quan."""
    try:
        # L·∫•y th√¥ng tin b√†i vi·∫øt
        article = df.loc[article_id]
    except KeyError:
        st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt.")
        if st.button("‚¨ÖÔ∏è Quay l·∫°i danh s√°ch"):
            st.session_state.current_view = "main"
            st.session_state.current_article_id = None
            st.rerun()
        return
    
    # C·∫≠p nh·∫≠t l·ªãch s·ª≠ ƒë·ªçc
    st.session_state.read_articles.add(article_id)
    if article_id not in st.session_state.reading_history:
        st.session_state.reading_history.insert(0, article_id)
    
    # CSS cho giao di·ªán
    st.markdown("""
        <style>
            /* ·∫®n sidebar khi m·ªü r·ªông */
            [data-testid="stSidebar"][aria-expanded="true"]{
                display: none;
            }
          
            /* Style cho n·ªôi dung b√†i vi·∫øt */
            .article-content, .fck_detail, .detail-content, .dt-news__content {
                font-size: 15px !important;
                line-height: 1.6 !important;
                margin: 0 !important;
                padding: 0 !important;
            }

            /* Style cho ·∫£nh trong b√†i vi·∫øt */
            .article-content img,
            .fck_detail img,
            .detail-content img,
            .dt-news__content img {
                max-width: 100% !important;
                height: auto !important;
                display: block !important;
                margin: 10px auto !important;
                border-radius: 8px !important;
                box-shadow: 0 1px 4px rgba(0, 0, 0, 0.08) !important;
            }
            
            /* Style cho li√™n k·∫øt */
            .article-content a,
            .fck_detail a,
            .detail-content a,
            .dt-news__content a {
                color: #0066cc !important;
                text-decoration: none !important;
                border-bottom: 1px solid transparent !important;
                transition: border-color 0.2s ease !important;
            }
            .article-content a:hover,
            .fck_detail a:hover,
            .detail-content a:hover,
            .dt-news__content a:hover {
                border-bottom-color: #0066cc !important;
            }
            
            /* Style cho blockquote */
            .article-content blockquote,
            .fck_detail blockquote,
            .detail-content blockquote,
            .dt-news__content blockquote {
                border-left: 4px solid #0066cc !important;
                margin: 1em 0 !important;
                padding: 0.5em 1em !important;
                background: #f8f9fa !important;
                font-style: italic !important;
            }
            
            /* Style cho ƒëo·∫°n vƒÉn */
            .article-content p,
            .fck_detail p,
            .detail-content p,
            .dt-news__content p {
                margin: 1em 0 !important;
            }
        </style>
    """, unsafe_allow_html=True)

    # N√∫t quay l·∫°i
    if st.button("‚¨ÖÔ∏è Quay l·∫°i danh s√°ch"):
        st.session_state.current_view = "main"
        st.session_state.current_article_id = None
        st.rerun()
    
    # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ v√† th√¥ng tin b√†i vi·∫øt
    st.title(article['title'])
    
    # Gi·ªØ nguy√™n th·ªùi gian t·ª´ RSS (ƒë√£ ƒë√∫ng m√∫i gi·ªù Vi·ªát Nam)
    vn_time = article['published_time']
    
    st.caption(f"Ngu·ªìn: {article['source_name']} | Xu·∫•t b·∫£n: {vn_time.strftime('%d-%m-%Y %H:%M')}")
    st.markdown("---")
    
    # Chia layout th√†nh 2 c·ªôt
    col1, col2 = st.columns([0.6, 0.4])
    
    # C·ªôt tr√°i: N·ªôi dung b√†i vi·∫øt
    with col1:
        # Hi·ªÉn th·ªã ·∫£nh ƒë·∫°i di·ªán
        if pd.notna(article['image_url']):
            st.markdown(f'<img src="{article["image_url"]}" onerror="this.onerror=null; this.src=\'no-image-png-2.webp\';" style="width:100%;">', unsafe_allow_html=True)
        else:
            st.markdown('<img src="no-image-png-2.webp" style="width:100%;">', unsafe_allow_html=True)
        
        # T·∫£i v√† hi·ªÉn th·ªã n·ªôi dung b√†i vi·∫øt
        with st.spinner("ƒêang t·∫£i n·ªôi dung b√†i vi·∫øt..."):
            article_content = crawl_article_content(article['link'])
            if article_content:
                # B·ªçc n·ªôi dung trong container
                article_content = f'<div class="article-content">{article_content}</div>'
                st.markdown(article_content, unsafe_allow_html=True)
            else:
                # Hi·ªÉn th·ªã t√≥m t·∫Øt n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung ƒë·∫ßy ƒë·ªß
                st.warning("Kh√¥ng th·ªÉ t·∫£i n·ªôi dung ƒë·∫ßy ƒë·ªß c·ªßa b√†i vi·∫øt. Hi·ªÉn th·ªã t√≥m t·∫Øt thay th·∫ø.")
                summary_raw = article.get('summary_raw', '')
                summary_without_img = re.sub(r'<img[^>]*>', '', summary_raw, flags=re.IGNORECASE)
                st.markdown(f'<div class="article-content">{summary_without_img}</div>', unsafe_allow_html=True)
        
        # Link ƒë·∫øn b√†i vi·∫øt g·ªëc
        st.link_button("ƒê·ªçc to√†n b·ªô b√†i vi·∫øt tr√™n trang g·ªëc", article['link'])
    
    # C·ªôt ph·∫£i: B√†i vi·∫øt li√™n quan
    with col2:
        st.subheader("Kh√°m ph√° th√™m")
        # Cho ph√©p ch·ªçn lo·∫°i b√†i vi·∫øt li√™n quan
        rec_type = st.radio("Hi·ªÉn th·ªã c√°c b√†i vi·∫øt:", ("C√≥ n·ªôi dung t∆∞∆°ng t·ª±", "Trong c√πng ch·ªß ƒë·ªÅ"), key=f"rec_type_{article_id}")
        
        if rec_type == "C√≥ n·ªôi dung t∆∞∆°ng t·ª±":
            # Hi·ªÉn th·ªã b√†i vi·∫øt t∆∞∆°ng t·ª± d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a
            st.markdown("##### D·ª±a tr√™n ph√¢n t√≠ch ng·ªØ nghƒ©a:")
            sim_scores = sorted(list(enumerate(cosine_sim[article_id])), key=lambda x: x[1], reverse=True)[1:6]
            for i, (article_index, score) in enumerate(sim_scores):
                rec_article = df.iloc[article_index]
                with st.container(border=True):
                    rec_col1, rec_col2 = st.columns([0.25, 0.75])
                    with rec_col1:
                        if pd.notna(rec_article['image_url']):
                            st.markdown(f'<img src="{rec_article["image_url"]}" onerror="this.onerror=null; this.src=\'no-image-png-2.webp\';" style="width:100%;">', unsafe_allow_html=True)
                        else:
                            st.markdown('<img src="no-image-png-2.webp" style="width:100%;">', unsafe_allow_html=True)
                    with rec_col2:
                        if st.button(rec_article['title'], key=f"rec_{article_index}"):
                            # C·∫≠p nh·∫≠t l·ªãch s·ª≠ ƒë·ªçc cho b√†i vi·∫øt ƒë∆∞·ª£c ch·ªçn
                            st.session_state.read_articles.add(article_index)
                            if article_index not in st.session_state.reading_history:
                                st.session_state.reading_history.insert(0, article_index)
                            st.session_state.current_article_id = article_index
                            st.rerun()
                        st.caption(f"Ngu·ªìn: {rec_article['source_name']} | ƒê·ªô t∆∞∆°ng ƒë·ªìng: {score:.2f}")
        else:
            # Hi·ªÉn th·ªã b√†i vi·∫øt c√πng ch·ªß ƒë·ªÅ
            cluster_id = article['topic_cluster']
            topic_name = topic_labels.get(str(cluster_id), "N/A")
            st.markdown(f"##### Thu·ªôc ch·ªß ƒë·ªÅ: **{topic_name}**")
            same_cluster_df = df[(df['topic_cluster'] == cluster_id) & (df.index != article_id)].head(5)
            for i, row in same_cluster_df.iterrows():
                with st.container(border=True):
                    rec_col1, rec_col2 = st.columns([0.25, 0.75])
                    with rec_col1:
                        if pd.notna(row['image_url']):
                            st.markdown(f'<img src="{row["image_url"]}" onerror="this.onerror=null; this.src=\'no-image-png-2.webp\';" style="width:100%;">', unsafe_allow_html=True)
                        else:
                            st.markdown('<img src="no-image-png-2.webp" style="width:100%;">', unsafe_allow_html=True)
                    with rec_col2:
                        if st.button(row['title'], key=f"rec_{i}"):
                            # C·∫≠p nh·∫≠t l·ªãch s·ª≠ ƒë·ªçc cho b√†i vi·∫øt ƒë∆∞·ª£c ch·ªçn
                            st.session_state.read_articles.add(i)
                            if i not in st.session_state.reading_history:
                                st.session_state.reading_history.insert(0, i)
                            st.session_state.current_article_id = i
                            st.rerun()
                        similarity_score = cosine_sim[article_id][i]
                        st.caption(f"Ngu·ªìn: {row['source_name']} | ƒê·ªô t∆∞∆°ng ƒë·ªìng: {similarity_score:.2f}")

def render_search_results(query, df, embeddings, sbert_model):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ t√¨m ki·∫øm."""
    st.header(f"K·∫øt qu·∫£ t√¨m ki·∫øm cho: \"{query}\"")
    # Vector h√≥a c√¢u truy v·∫•n
    with st.spinner("ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm..."):
        query_vector = sbert_model.encode([query])
        # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng
        similarities = cosine_similarity(query_vector, embeddings)[0]
        # S·∫Øp x·∫øp v√† l·∫•y k·∫øt qu·∫£
        sim_scores = sorted(list(enumerate(similarities)), key=lambda x: x[1], reverse=True)
        result_indices = [i[0] for i in sim_scores]
        result_df = df.iloc[result_indices].copy()
    render_main_grid(result_df, f"K·∫øt qu·∫£ cho: \"{query}\"")

# ==== C√ÅC H√ÄM HI·ªÇN TH·ªä (RENDER) ---
def render_main_grid(df, selected_topic_name):
    """Hi·ªÉn th·ªã l∆∞·ªõi b√†i vi·∫øt ch√≠nh."""
    st.header(f"B·∫£ng tin: {selected_topic_name}")
    st.markdown(f"T√¨m th·∫•y **{len(df)}** b√†i vi·∫øt li√™n quan.")
    st.markdown("---")
    
    # Chia layout th√†nh 3 c·ªôt
    num_columns = 3
    cols = st.columns(num_columns)
    
    if df.empty:
        st.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ph√π h·ª£p v·ªõi l·ª±a ch·ªçn c·ªßa b·∫°n.")
    else:
        # Hi·ªÉn th·ªã t·ª´ng b√†i vi·∫øt
        for i, (index, row) in enumerate(df.iterrows()):
            with cols[i % num_columns]:
                # X·ª≠ l√Ω h√¨nh ·∫£nh
                image_html = ''
                if pd.notna(row["image_url"]):
                    image_html = f'<div class="card-image-container"><img src="{row["image_url"]}" onerror="this.onerror=null; this.src=\'no-image-png-2.webp\';"></div>'
                else:
                    image_html = '<div class="card-image-container"><img src="no-image-png-2.webp"></div>'
                
                # T·∫°o card b√†i vi·∫øt
                card_html = f"""<div class="article-card">
                                {image_html}
                                <div class="article-content">
                                    <div class="article-title">{row['title']}</div>
                                    <div class="article-source">{row['source_name']}</div>
                                </div>
                           </div>"""
                st.markdown(card_html, unsafe_allow_html=True)
                
                # N√∫t ƒë·ªçc b√†i vi·∫øt
                if st.button("ƒê·ªçc b√†i vi·∫øt", key=f"read_{index}"):
                    st.session_state.current_article_id = index
                    st.session_state.current_view = "detail"
                    st.rerun()

# --- LU·ªíNG CH√çNH C·ª¶A ·ª®NG D·ª§NG ---
local_css("style.css")

# --- PH·∫¶N LOGIC M·ªöI: QU·∫¢N L√ù TR·∫†NG TH√ÅI ---
if 'update_log' not in st.session_state:
    st.session_state.update_log = ""
if 'update_error' not in st.session_state:
    st.session_state.update_error = ""
if 'update_success' not in st.session_state:
    st.session_state.update_success = False

# Initialize session state for reading history and interest tracking
if 'reading_history' not in st.session_state:
    st.session_state.reading_history = []
if 'interest_vector' not in st.session_state:
    st.session_state.interest_vector = None
if 'interest_articles' not in st.session_state:
    st.session_state.interest_articles = None

df, cosine_sim, topic_labels, embeddings = load_data_and_embeddings()
sbert_model = get_sbert_model()

# --- √î T√åM KI·∫æM SEMANTIC ·ªû HEADER ---
search_col1, search_col2 = st.columns([0.85, 0.15])
with search_col1:
    search_input = st.text_input(
        "T√¨m ki·∫øm b√†i vi·∫øt (theo ng·ªØ nghƒ©a, nh·∫≠p t·ª´ kh√≥a ho·∫∑c c√¢u h·ªèi):",
        value=st.session_state.get('search_query', ''),
        key="search_input",
        placeholder="Nh·∫≠p n·ªôi dung b·∫°n mu·ªën t√¨m...",
        label_visibility="collapsed"
    )
with search_col2:
    search_button = st.button("üîç T√¨m ki·∫øm", use_container_width=True)

if search_input and (search_button or search_input != st.session_state.get('search_query', '')):
    st.session_state['search_query'] = search_input
    st.session_state['current_view'] = "search"
    st.rerun()

if st.session_state.get('current_view', 'main') == "search" and st.session_state.get('search_query', ''):
    with st.spinner("ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm..."):
        query_vector = sbert_model.encode([st.session_state['search_query']])
        similarities = cosine_similarity(query_vector, embeddings)[0]
        sim_scores = sorted(list(enumerate(similarities)), key=lambda x: x[1], reverse=True)
        result_indices = [i[0] for i in sim_scores]
        result_df = df.iloc[result_indices].copy()
    st.sidebar.info("B·∫°n ƒëang ·ªü trang k·∫øt qu·∫£ t√¨m ki·∫øm. Ch·ªçn danh m·ª•c kh√°c ho·∫∑c b·∫•m 'Quay l·∫°i' ƒë·ªÉ tr·ªü v·ªÅ.")
    if st.sidebar.button("‚¨ÖÔ∏è Quay l·∫°i trang ch·ªß", use_container_width=True):
        st.session_state['search_query'] = ''
        st.session_state['current_view'] = "main"
        st.rerun()
    render_main_grid(result_df, f"K·∫øt qu·∫£ cho: \"{st.session_state['search_query']}\"")
    st.stop()
elif st.session_state.current_view == "detail" and st.session_state.current_article_id is not None:
    render_detail_view(st.session_state.current_article_id, df, cosine_sim, topic_labels)
else:
    # --- GIAO DI·ªÜN THANH B√äN ---
    st.sidebar.title("T·∫°p ch√≠ c·ªßa b·∫°n")
    st.sidebar.markdown("---")

    # N√∫t c·∫≠p nh·∫≠t d·ªØ li·ªáu
    if st.sidebar.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu", use_container_width=True):
        with st.spinner("‚è≥ ƒêang t·∫£i l·∫°i d·ªØ li·ªáu..."):
            try:
                # X√≥a cache ƒë·ªÉ bu·ªôc t·∫£i l·∫°i d·ªØ li·ªáu t·ª´ CSV
                st.cache_data.clear()
                st.session_state.update_log = "ƒê√£ x√≥a cache v√† t·∫£i l·∫°i d·ªØ li·ªáu t·ª´ final_articles_for_app.csv"
                st.session_state.update_error = ""
                st.session_state.update_success = True
            except Exception as e:
                st.session_state.update_error = f"L·ªói khi t·∫£i l·∫°i d·ªØ li·ªáu: {e}"
                st.session_state.update_success = False

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ c·∫≠p nh·∫≠t v√† n√∫t t·∫£i l·∫°i
    if st.session_state.update_success:
        st.sidebar.success("‚úÖ ƒê√£ t·∫£i l·∫°i d·ªØ li·ªáu!")
        with st.sidebar.expander("Xem chi ti·∫øt"):
            st.info(st.session_state.update_log)
            if st.session_state.update_error:
                st.error("L·ªói:")
                st.code(st.session_state.update_error)
        if st.sidebar.button("Xem d·ªØ li·ªáu m·ªõi", use_container_width=True):
            st.session_state.update_success = False # Reset c·ªù
            st.rerun()

    st.sidebar.markdown("---")

    if df is None:
        st.error("L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu. Vui l√≤ng b·∫•m n√∫t 'C·∫≠p nh·∫≠t d·ªØ li·ªáu' ·ªü thanh b√™n.")
    else:
        # --- PH·∫¶N L·ªåC THEO CH·ª¶ ƒê·ªÄ ---
        st.sidebar.subheader("Kh√°m ph√° c√°c ch·ªß ƒë·ªÅ")
        topic_display_list = ["D√†nh cho b·∫°n (T·∫•t c·∫£)", "B√†i vi·∫øt ƒë√£ ƒë·ªçc", "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc"] + [v for k, v in topic_labels.items()]
        
        st.sidebar.markdown('<div class="sidebar-nav">', unsafe_allow_html=True)
        for topic in topic_display_list:
            is_active = (topic == st.session_state.selected_topic)
            active_class = "active" if is_active else ""
            icon = "üìñ" if topic != "B√†i vi·∫øt ƒë√£ ƒë·ªçc" and topic != "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc" else "üëÄ"if topic == "B√†i vi·∫øt ƒë√£ ƒë·ªçc" else "üéØ"
            if st.sidebar.button(f"{icon} {topic}", key=f"topic_{topic}", use_container_width=True):
                st.session_state.selected_topic = topic
                st.rerun()
        st.sidebar.markdown('</div>', unsafe_allow_html=True)
        st.sidebar.markdown("---")

        # --- B·ªî SUNG: PH·∫¶N L·ªåC THEO NGU·ªíN ---
        st.sidebar.subheader("L·ªçc theo ngu·ªìn")
        all_sources = sorted(df['source_name'].unique().tolist())
        selected_sources = st.sidebar.multiselect(
            "Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu ngu·ªìn:",
            options=all_sources,
            default=st.session_state.selected_sources
        )
        
        if selected_sources != st.session_state.selected_sources:
            st.session_state.selected_sources = selected_sources
            st.rerun()

        # --- HI·ªÇN TH·ªä VIEW T∆Ø∆†NG ·ª®NG ---
        if st.session_state.selected_topic == "B√†i vi·∫øt ƒë√£ ƒë·ªçc":
            if st.session_state.read_articles:
                # L·∫•y danh s√°ch b√†i vi·∫øt ƒë√£ ƒë·ªçc theo th·ª© t·ª± trong reading_history (m·ªõi nh·∫•t l√™n ƒë·∫ßu)
                ordered_articles = [article_id for article_id in st.session_state.reading_history if article_id in st.session_state.read_articles]
                # T·∫°o DataFrame v·ªõi th·ª© t·ª± ƒë√£ s·∫Øp x·∫øp
                display_df = df[df.index.isin(ordered_articles)].copy()
                # S·∫Øp x·∫øp l·∫°i theo th·ª© t·ª± trong ordered_articles
                display_df = display_df.reindex(ordered_articles)
            else:
                display_df = pd.DataFrame()
                st.info("B·∫°n ch∆∞a ƒë·ªçc b√†i vi·∫øt n√†o.")
        elif st.session_state.selected_topic == "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc":
            if len(st.session_state.reading_history) > 0:
                display_df = get_similar_articles_by_history(
                    df, cosine_sim,
                    st.session_state.reading_history,
                    exclude_articles=st.session_state.read_articles
                )
                if display_df.empty:
                    st.info("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt t∆∞∆°ng t·ª± d·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc.")
            else:
                display_df = pd.DataFrame()
                st.info("B·∫°n ch∆∞a c√≥ l·ªãch s·ª≠ ƒë·ªçc b√†i vi·∫øt n√†o.")
        elif st.session_state.selected_topic != "D√†nh cho b·∫°n (T·∫•t c·∫£)":
            selected_key_list = [k for k, v in topic_labels.items() if v == st.session_state.selected_topic]
            if selected_key_list:
                display_df = df[df['topic_cluster'] == int(selected_key_list[0])].copy()
            else:
                display_df = pd.DataFrame()
        else:
            display_df = df.copy()

        # √Åp d·ª•ng b·ªô l·ªçc ngu·ªìn
        if st.session_state.selected_sources:
            display_df = display_df[display_df['source_name'].isin(st.session_state.selected_sources)]

        # S·∫Øp x·∫øp v√† hi·ªÉn th·ªã
        if not display_df.empty:
            # Ch·ªâ s·∫Øp x·∫øp theo th·ªùi gian ƒëƒÉng n·∫øu kh√¥ng ph·∫£i l√† b√†i vi·∫øt ƒë√£ ƒë·ªçc
            if st.session_state.selected_topic != "B√†i vi·∫øt ƒë√£ ƒë·ªçc":
                # S·∫Øp x·∫øp ƒë∆°n gi·∫£n nh∆∞ Excel - so s√°nh tr·ª±c ti·∫øp datetime
                display_df = display_df.sort_values(by='published_time', ascending=False, kind='mergesort')
        render_main_grid(display_df, st.session_state.selected_topic)

