import feedparser
import pandas as pd
import numpy as np
import json
import time
import warnings
import sys
import os # ThÃªm thÆ° viá»‡n os
from dotenv import load_dotenv

# --- Sá»¬A Lá»–I SUBPROCESS/JOBLIB TRÃŠN WINDOWS ---
# Äáº·t má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh cho sá»‘ lÃµi CPU Ä‘á»ƒ trÃ¡nh lá»—i khi joblib/loky
# cá»‘ gáº¯ng tá»± Ä‘á»™ng Ä‘áº¿m lÃµi trong má»™t sá»‘ mÃ´i trÆ°á»ng phá»©c táº¡p.
os.environ['LOKY_MAX_CPU_COUNT'] = '4'

from bs4 import BeautifulSoup
from dateutil.parser import parse as parse_date
from datetime import datetime, timedelta, timezone
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
from urllib.parse import urlparse

# --- Cáº¤U HÃŒNH -----------------------------------------------------------------
warnings.filterwarnings('ignore')

# Load environment variables from .env file
load_dotenv()

# >>> Sá»¬A Lá»–I UNICODE TRÃŠN WINDOWS CONSOLE <<<
# Buá»™c output cá»§a chÆ°Æ¡ng trÃ¬nh pháº£i lÃ  UTF-8 Ä‘á»ƒ hiá»ƒn thá»‹ tiáº¿ng Viá»‡t chÃ­nh xÃ¡c
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except TypeError: # Bá» qua náº¿u cháº¡y trÃªn mÃ´i trÆ°á»ng khÃ´ng há»— trá»£
    pass


# CÃ¡c nguá»“n RSS Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u
RSS_URLS = [
    "https://thanhnien.vn/rss/home.rss",
    "https://tuoitre.vn/rss/tin-moi-nhat.rss",
    "https://vnexpress.net/rss/tin-moi-nhat.rss",
]

# Cáº¥u hÃ¬nh cho cÃ¡c mÃ´ hÃ¬nh
NUM_CLUSTERS = 12
SBERT_MODEL = 'vinai/phobert-base-v2'

# --- KIá»‚M TRA VÃ€ Cáº¤U HÃŒNH API KEY (PHáº¦N Gá»  Lá»–I) ----------------------------
print("--- KIá»‚M TRA API KEY ---")
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y API Key trong biáº¿n mÃ´i trÆ°á»ng.")
    print("Vui lÃ²ng táº¡o file .env vÃ  thÃªm dÃ²ng sau:")
    print("GOOGLE_API_KEY=your_api_key_here")
    exit()  # Dá»«ng chÆ°Æ¡ng trÃ¬nh ngay láº­p tá»©c
else:
    # In ra má»™t pháº§n cá»§a key Ä‘á»ƒ xÃ¡c nháº­n
    print(f"âœ… ÄÃ£ tÃ¬m tháº¥y API Key. Báº¯t Ä‘áº§u báº±ng: '{api_key[:4]}...'. Káº¿t thÃºc báº±ng: '...{api_key[-4:]}'.")
    print("Äang cáº¥u hÃ¬nh vá»›i Google...")
    try:
        genai.configure(api_key=api_key)
        print("âœ… Cáº¥u hÃ¬nh Google API thÃ nh cÃ´ng.")
    except Exception as e:
        print(f"âŒ Lá»–I KHI Cáº¤U HÃŒNH: {e}")
        # KhÃ´ng thoÃ¡t, Ä‘á»ƒ hÃ m generate_meaningful_topic_name xá»­ lÃ½ lá»—i vÃ  tiáº¿p tá»¥c
        pass


# --- CÃC HÃ€M CHá»¨C NÄ‚NG (Giá»¯ nguyÃªn) ----------------------------------------

def get_source_name(link):
    """TrÃ­ch xuáº¥t tÃªn miá»n chÃ­nh tá»« URL Ä‘á»ƒ lÃ m tÃªn nguá»“n."""
    try:
        domain = urlparse(link).netloc
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain.split('.')[0].capitalize()
    except:
        return "N/A"

def fetch_recent_articles(hours=24):
    """Láº¥y cÃ¡c bÃ i viáº¿t má»›i tá»« RSS vÃ  trÃ­ch xuáº¥t URL hÃ¬nh áº£nh, tÃªn nguá»“n."""
    print(f"\n1/6: Báº¯t Ä‘áº§u láº¥y cÃ¡c bÃ i viáº¿t trong vÃ²ng {hours} giá» qua...")
    articles = []
    # Sá»­ dá»¥ng mÃºi giá» Viá»‡t Nam (UTC+7)
    vn_timezone = timezone(timedelta(hours=7))
    time_threshold = datetime.now(vn_timezone) - timedelta(hours=hours)
    for url in RSS_URLS:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            published_time = entry.get("published", "")
            image_url = None
            summary_raw = entry.get("summary", "")
            if summary_raw:
                soup = BeautifulSoup(summary_raw, 'html.parser')
                img_tag = soup.find('img')
                if img_tag and 'src' in img_tag.attrs:
                    image_url = img_tag['src']
            
            # TrÃ­ch xuáº¥t tÃªn nguá»“n tá»« link bÃ i viáº¿t
            source_name = get_source_name(entry.link)

            if published_time:
                try:
                    # Parse thá»i gian vÃ  chuyá»ƒn sang mÃºi giá» Viá»‡t Nam
                    parsed_time = parse_date(published_time).astimezone(vn_timezone)
                    if parsed_time >= time_threshold:
                        articles.append({
                            "title": entry.title, "link": entry.link,
                            "summary_raw": summary_raw, "published_time": parsed_time.isoformat(),
                            "image_url": image_url,
                            "source": source_name
                        })
                except (ValueError, TypeError):
                    continue
    print(f"-> ÄÃ£ tÃ¬m tháº¥y {len(articles)} bÃ i viáº¿t má»›i.")
    return pd.DataFrame(articles)

def clean_text(df):
    """LÃ m sáº¡ch vÄƒn báº£n tÃ³m táº¯t."""
    print("2/6: Äang lÃ m sáº¡ch vÄƒn báº£n...")
    df['summary_cleaned'] = df['summary_raw'].str.lower().str.replace(r'<.*?>', '', regex=True)
    df['summary_cleaned'] = df['summary_cleaned'].str.replace(r'[^\w\s]', '', regex=True)
    df.dropna(subset=['summary_cleaned'], inplace=True)
    df = df[df['summary_cleaned'].str.strip() != ''].reset_index(drop=True)
    print("-> LÃ m sáº¡ch vÄƒn báº£n hoÃ n táº¥t.")
    return df

def vectorize_text(sentences, model_name):
    """Vector hÃ³a cÃ¢u báº±ng S-BERT."""
    print(f"3/6: Äang vector hÃ³a vÄƒn báº£n báº±ng mÃ´ hÃ¬nh {model_name}...")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(sentences, show_progress_bar=True)
    print("-> Vector hÃ³a hoÃ n táº¥t.")
    return embeddings

def generate_meaningful_topic_name(keywords, sample_titles):
    """Sá»­ dá»¥ng Gemini Ä‘á»ƒ táº¡o tÃªn chá»§ Ä‘á»."""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        prompt = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ biÃªn táº­p bÃ¡o chÃ­. Dá»±a vÃ o cÃ¡c thÃ´ng tin dÆ°á»›i Ä‘Ã¢y, hÃ£y táº¡o ra má»™t tÃªn chá»§ Ä‘á» ngáº¯n gá»n (khÃ´ng quÃ¡ 6 tá»«) báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ tÃ³m táº¯t ná»™i dung chÃ­nh.
        CÃ¡c tá»« khÃ³a chÃ­nh cá»§a chá»§ Ä‘á»: {keywords}
        Má»™t vÃ i tiÃªu Ä‘á» bÃ i viáº¿t vÃ­ dá»¥:
        - {"\n- ".join(sample_titles)}
        TÃªn chá»§ Ä‘á» gá»£i Ã½:"""
        response = model.generate_content(prompt)
        return response.text.strip().replace("*", "")
    except Exception as e:
        # In ra lá»—i cá»¥ thá»ƒ hÆ¡n
        print(f"  - Lá»—i khi gá»i Gemini API: {type(e).__name__} - {e}. Sá»­ dá»¥ng tá»« khÃ³a lÃ m nhÃ£n thay tháº¿.")
        return keywords

def get_topic_labels(df, num_keywords=5):
    """GÃ¡n nhÃ£n chá»§ Ä‘á» cho cÃ¡c cá»¥m."""
    print("4/6: Äang gÃ¡n nhÃ£n chá»§ Ä‘á» cho cÃ¡c cá»¥m...")
    topic_labels = {}
    for i in range(NUM_CLUSTERS):
        cluster_df = df[df['topic_cluster'] == i]
        cluster_texts = cluster_df['summary_cleaned'].tolist()
        if len(cluster_texts) < 3:
            topic_labels[str(i)] = "Chá»§ Ä‘á» nhá» (Ã­t bÃ i viáº¿t)"
            continue
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(cluster_texts)
        avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1
        top_indices = avg_tfidf_scores.argsort()[-num_keywords:][::-1]
        feature_names = vectorizer.get_feature_names_out()
        keywords = ", ".join([feature_names[j] for j in top_indices])
        sample_titles = cluster_df['title'].head(3).tolist()
        meaningful_name = generate_meaningful_topic_name(keywords, sample_titles)
        print(f"  - Cluster {i}: {keywords}  =>  TÃªn chá»§ Ä‘á»: {meaningful_name}")
        topic_labels[str(i)] = meaningful_name
        time.sleep(1)
    print("-> GÃ¡n nhÃ£n chá»§ Ä‘á» hoÃ n táº¥t.")
    return topic_labels

def main_pipeline():
    """HÃ m chÃ­nh cháº¡y toÃ n bá»™ quy trÃ¬nh."""
    print("\nğŸš€ Báº®T Äáº¦U QUY TRÃŒNH Tá»° Äá»˜NG HÃ“A")
    
    # CÃ¡c bÆ°á»›c giá»¯ nguyÃªn
    df = fetch_recent_articles(hours=24)
    if df.empty:
        print("KhÃ´ng cÃ³ bÃ i viáº¿t má»›i nÃ o. Dá»«ng quy trÃ¬nh.")
        return
    df = clean_text(df)
    embeddings = vectorize_text(df['summary_cleaned'].tolist(), SBERT_MODEL)
    
    print("Äang thá»±c hiá»‡n phÃ¢n cá»¥m...")
    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)
    df['topic_cluster'] = kmeans.fit_predict(embeddings)
    
    topic_labels = get_topic_labels(df)
    
    print("5/6: Äang tÃ­nh toÃ¡n ma tráº­n tÆ°Æ¡ng Ä‘á»“ng...")
    cosine_sim = cosine_similarity(embeddings)
    
    print("6/6: Äang lÆ°u cÃ¡c file káº¿t quáº£...")
    df.to_csv('final_articles_for_app.csv', index=False, encoding='utf-8-sig')
    np.save('cosine_similarity_matrix.npy', cosine_sim)
    with open('topic_labels.json', 'w', encoding='utf-8') as f:
        json.dump(topic_labels, f, ensure_ascii=False, indent=4)
    
    print("\nâœ… QUY TRÃŒNH HOÃ€N Táº¤T! âœ…")

if __name__ == "__main__":
    main_pipeline()
