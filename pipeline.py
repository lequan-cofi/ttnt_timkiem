import feedparser
import pandas as pd
import numpy as np
import json
import time
import warnings
import sys
import os # ThÃªm thÆ° viá»‡n os
from dotenv import load_dotenv
import html
from sklearn.metrics import silhouette_score
import numpy as np
# XÃ³a API key cÅ© tá»« biáº¿n mÃ´i trÆ°á»ng náº¿u cÃ³
if 'GOOGLE_API_KEY' in os.environ:
    del os.environ['GOOGLE_API_KEY']

# --- Sá»¬A Lá»–I SUBPROCESS/JOBLIB TRÃŠN WINDOWS ---
# Äáº·t má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh cho sá»‘ lÃµi CPU Ä‘á»ƒ trÃ¡nh lá»—i khi joblib/loky
# cá»‘ gáº¯ng tá»± Ä‘á»™ng Ä‘áº¿m lÃµi trong má»™t sá»‘ mÃ´i trÆ°á»ng phá»©c táº¡p.
os.environ['LOKY_MAX_CPU_COUNT'] = '4'

from bs4 import BeautifulSoup
from dateutil.parser import parse as parse_date
from datetime import datetime, timedelta, timezone
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
from urllib.parse import urlparse
from pyvi import ViTokenizer
from stopwordsiso import stopwords
# --- Cáº¤U HÃŒNH -----------------------------------------------------------------
warnings.filterwarnings('ignore')

# Load environment variables from .env file
load_dotenv()

# Debug print to verify API key and environment
print(f"Current working directory: {os.getcwd()}")
print(f"Environment file path: {os.path.join(os.getcwd(), '.env')}")
print(f"Environment variables: {dict(os.environ)}")

# >>> Sá»¬A Lá»–I UNICODE TRÃŠN WINDOWS CONSOLE <<<
# Buá»™c output cá»§a chÆ°Æ¡ng trÃ¬nh pháº£i lÃ  UTF-8 Ä‘á»ƒ hiá»ƒn thá»‹ tiáº¿ng Viá»‡t chÃ­nh xÃ¡c
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except TypeError: # Bá» qua náº¿u cháº¡y trÃªn mÃ´i trÆ°á»ng khÃ´ng há»— trá»£
    pass


# CÃ¡c nguá»“n RSS Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u
RSS_URLS = [
    "https://dantri.com.vn/rss/home.rss",
    "https://dantri.com.vn/rss/xa-hoi.rss",
    "https://dantri.com.vn/rss/gia-vang.rss",
    "https://dantri.com.vn/rss/the-thao.rss",
    "https://dantri.com.vn/rss/giao-duc.rss",
    "https://dantri.com.vn/rss/kinh-doanh.rss",
    "https://dantri.com.vn/rss/giai-tri.rss",
    "https://dantri.com.vn/rss/phap-luat.rss",
    "https://dantri.com.vn/rss/cong-nghe.rss",
    "https://dantri.com.vn/rss/tinh-yeu-gioi-tinh.rss",
    "https://dantri.com.vn/rss/noi-vu.rss",
    "https://dantri.com.vn/rss/tam-diem.rss",
    "https://dantri.com.vn/rss/infographic.rss",
    "https://dantri.com.vn/rss/dnews.rss",
    "https://dantri.com.vn/rss/xo-so.rss",
    "https://dantri.com.vn/rss/tet-2025.rss",
    "https://dantri.com.vn/rss/d-buzz.rss",
    "https://dantri.com.vn/rss/su-kien.rss",
    "https://dantri.com.vn/rss/the-gioi.rss",
    "https://dantri.com.vn/rss/doi-song.rss",
    "https://dantri.com.vn/rss/lao-dong-viec-lam.rss",
    "https://dantri.com.vn/rss/tam-long-nhan-ai.rss",
    "https://dantri.com.vn/rss/bat-dong-san.rss",
    "https://dantri.com.vn/rss/du-lich.rss",
    "https://dantri.com.vn/rss/suc-khoe.rss",
    "https://dantri.com.vn/rss/o-to-xe-may.rss",
    "https://dantri.com.vn/rss/khoa-hoc.rss",
    "https://dantri.com.vn/rss/ban-doc.rss",
    "https://dantri.com.vn/rss/dmagazine.rss",
    "https://dantri.com.vn/rss/photo-news.rss",
    "https://dantri.com.vn/rss/toa-dam-truc-tuyen.rss",
    "https://dantri.com.vn/rss/interactive.rss",
    "https://dantri.com.vn/rss/photo-story.rss",
    "https://vnexpress.net/rss/tin-moi-nhat.rss",      # Trang chá»§ (thÆ°á»ng lÃ  tin má»›i nháº¥t)
    "https://vnexpress.net/rss/the-gioi.rss",
    "https://vnexpress.net/rss/thoi-su.rss",
    "https://vnexpress.net/rss/kinh-doanh.rss",
    "https://vnexpress.net/rss/startup.rss",
    "https://vnexpress.net/rss/giai-tri.rss",
    "https://vnexpress.net/rss/the-thao.rss",
    "https://vnexpress.net/rss/phap-luat.rss",
    "https://vnexpress.net/rss/giao-duc.rss",
    "https://vnexpress.net/rss/tin-noi-bat.rss",
    
    # Cá»™t bÃªn pháº£i
    "https://vnexpress.net/rss/suc-khoe.rss",
    "https://vnexpress.net/rss/doi-song.rss",
    "https://vnexpress.net/rss/du-lich.rss",
    "https://vnexpress.net/rss/khoa-hoc.rss",         # Khoa há»c cÃ´ng nghá»‡
    "https://vnexpress.net/rss/xe.rss",
    "https://vnexpress.net/rss/y-kien.rss",
    "https://vnexpress.net/rss/tam-su.rss",
    "https://vnexpress.net/rss/cuoi.rss",
    "https://vnexpress.net/rss/tin-xem-nhieu.rss",
    "https://thanhnien.vn/rss/home.rss",                   # Trang chá»§
    "https://thanhnien.vn/rss/thoi-su.rss",
    "https://thanhnien.vn/rss/chinh-tri.rss",
    "https://thanhnien.vn/rss/chao-ngay-moi.rss",
    "https://thanhnien.vn/rss/the-gioi.rss",
    "https://thanhnien.vn/rss/kinh-te.rss",
    "https://thanhnien.vn/rss/doi-song.rss",
    "https://thanhnien.vn/rss/suc-khoe.rss",
    "https://thanhnien.vn/rss/gioi-tre.rss",
    "https://thanhnien.vn/rss/tieu-dung-thong-minh.rss",
    "https://thanhnien.vn/rss/giao-duc.rss",
    "https://thanhnien.vn/rss/du-lich.rss",
    "https://thanhnien.vn/rss/van-hoa.rss",
    "https://thanhnien.vn/rss/giai-tri.rss",
    "https://thanhnien.vn/rss/the-thao.rss",
    "https://thanhnien.vn/rss/cong-nghe.rss",
    "https://thanhnien.vn/rss/xe.rss",
    "https://thanhnien.vn/rss/thoi-trang-tre.rss",
    "https://thanhnien.vn/rss/ban-doc.rss",
    "https://thanhnien.vn/rss/rao-vat.rss",
    "https://thanhnien.vn/rss/video.rss",
    "https://thanhnien.vn/rss/dien-dan.rss",
    "https://thanhnien.vn/rss/podcast.rss",
    "https://thanhnien.vn/rss/nhat-ky-tet-viet.rss",
    "https://thanhnien.vn/rss/magazine.rss",
    "https://thanhnien.vn/rss/cung-con-di-tiep-cuoc-doi.rss",
    "https://thanhnien.vn/rss/ban-can-biet.rss",
    "https://thanhnien.vn/rss/cai-chinh.rss",
    "https://thanhnien.vn/rss/blog-phong-vien.rss",
    "https://thanhnien.vn/rss/toi-viet.rss",
    "https://thanhnien.vn/rss/viec-lam.rss",
    
     # Cá»™t bÃªn trÃ¡i
    "https://tuoitre.vn/rss/tin-moi-nhat.rss",  # Trang chá»§
    "https://tuoitre.vn/rss/the-gioi.rss",
    "https://tuoitre.vn/rss/kinh-doanh.rss",
    "https://tuoitre.vn/rss/xe.rss",
    "https://tuoitre.vn/rss/van-hoa.rss",
    "https://tuoitre.vn/rss/the-thao.rss",
    "https://tuoitre.vn/rss/khoa-hoc.rss",
    "https://tuoitre.vn/rss/gia-that.rss",
    "https://tuoitre.vn/rss/ban-doc.rss",
    "https://tuoitre.vn/rss/video.rss",

    # Cá»™t bÃªn pháº£i
    "https://tuoitre.vn/rss/thoi-su.rss",
    "https://tuoitre.vn/rss/phap-luat.rss",
    "https://tuoitre.vn/rss/cong-nghe.rss",
    "https://tuoitre.vn/rss/nhip-song-tre.rss",
    "https://tuoitre.vn/rss/giai-tri.rss",
    "https://tuoitre.vn/rss/giao-duc.rss",
    "https://tuoitre.vn/rss/suc-khoe.rss",
    "https://tuoitre.vn/rss/thu-gian.rss",
    "https://tuoitre.vn/rss/du-lich.rss"
]

# Cáº¥u hÃ¬nh cho cÃ¡c mÃ´ hÃ¬nhgit 
NUM_CLUSTERS = None  # Sáº½ Ä‘Æ°á»£c cáº­p nháº­t sau khi tÃ¬m K tá»‘i Æ°u
SBERT_MODEL = 'Cloyne/vietnamese-sbert-v3'

# --- KIá»‚M TRA VÃ€ Cáº¤U HÃŒNH API KEY (PHáº¦N Gá»  Lá»–I) ----------------------------
print("--- KIá»‚M TRA API KEY ---")
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y API Key trong biáº¿n mÃ´i trÆ°á»ng.")
    print("Vui lÃ²ng táº¡o file .env vÃ  thÃªm dÃ²ng sau:")
    print("GOOGLE_API_KEY=your_api_key_here")
    exit()  # Dá»«ng chÆ°Æ¡ng trÃ¬nh ngay láº­p tá»©c
else:
    # In ra má»™t pháº§n cá»§a key Ä‘á»ƒ xÃ¡c nháº­n
    print(f"âœ… ÄÃ£ tÃ¬m tháº¥y API Key. Báº¯t Ä‘áº§u báº±ng: '{api_key[:4]}...'. Káº¿t thÃºc báº±ng: '...{api_key[-4:]}'.")
    print("Äang cáº¥u hÃ¬nh vá»›i Google...")
    try:
        genai.configure(api_key=api_key)
        print("âœ… Cáº¥u hÃ¬nh Google API thÃ nh cÃ´ng.")
    except Exception as e:
        print(f"âŒ Lá»–I KHI Cáº¤U HÃŒNH: {e}")
        # KhÃ´ng thoÃ¡t, Ä‘á»ƒ hÃ m generate_meaningful_topic_name xá»­ lÃ½ lá»—i vÃ  tiáº¿p tá»¥c
        pass


# --- CÃC HÃ€M CHá»¨C NÄ‚NG (Giá»¯ nguyÃªn) ----------------------------------------

def get_source_name(link):
    """TrÃ­ch xuáº¥t tÃªn miá»n chÃ­nh tá»« URL Ä‘á»ƒ lÃ m tÃªn nguá»“n."""
    try:
        domain = urlparse(link).netloc
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain.split('.')[0].capitalize()
    except:
        return "N/A"

def normalize_title(title):
    return html.unescape(title).strip().lower()

def fetch_recent_articles(rss_urls, hours=24):
    """
    HÃ m nÃ y láº¥y cÃ¡c bÃ i viáº¿t má»›i tá»« danh sÃ¡ch cÃ¡c URL RSS trong 24h,
    lá»c bá» cÃ¡c bÃ i cÃ³ tiÃªu Ä‘á» hoáº·c link trÃ¹ng láº·p.

    Args:
        rss_urls (list): Danh sÃ¡ch cÃ¡c URL cá»§a RSS feed.
        hours (int): Sá»‘ giá» tÃ­nh tá»« hiá»‡n táº¡i Ä‘á»ƒ láº¥y bÃ i viáº¿t.
        
    Returns:
        pd.DataFrame: Má»™t DataFrame chá»©a thÃ´ng tin cÃ¡c bÃ i viáº¿t duy nháº¥t Ä‘Ã£ thu tháº­p Ä‘Æ°á»£c.
    """
    print(f"\n1. Báº¯t Ä‘áº§u láº¥y cÃ¡c bÃ i viáº¿t trong vÃ²ng {hours} giá» qua...")
    articles = []
    
    # Khá»Ÿi táº¡o set Ä‘á»ƒ lÆ°u cÃ¡c tiÃªu Ä‘á» vÃ  link Ä‘Ã£ gáº·p
    seen_titles = set()
    seen_links = set()
    
    # Äáº·t má»‘c thá»i gian giá»›i háº¡n (chá»‰ láº¥y bÃ i má»›i hÆ¡n má»‘c nÃ y)
    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours)
    
    # Láº·p qua tá»«ng URL trong danh sÃ¡ch
    failed_rss = []  # Danh sÃ¡ch RSS lá»—i
    successful_rss = []  # Danh sÃ¡ch RSS thÃ nh cÃ´ng
    
    for url in rss_urls:
        try:
            print(f"  - Äang xá»­ lÃ½: {url}")
            feed = feedparser.parse(url)
            
            # Kiá»ƒm tra feed cÃ³ há»£p lá»‡ khÃ´ng
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                failed_rss.append(f"{url} - KhÃ´ng cÃ³ bÃ i viáº¿t hoáº·c RSS khÃ´ng há»£p lá»‡")
                print(f"    âŒ Lá»—i: KhÃ´ng cÃ³ bÃ i viáº¿t")
                continue
                
            entry_count = 0
            for entry in feed.entries:
                norm_title = normalize_title(entry.title)
                link = entry.link.strip()
                # Kiá»ƒm tra trÃ¹ng láº·p cáº£ tiÃªu Ä‘á» vÃ  link
                if norm_title in seen_titles or link in seen_links:
                    continue
                published_time = entry.get("published", "")
                summary_raw = entry.get("summary", "")
                image_url = None
                # TrÃ­ch xuáº¥t URL hÃ¬nh áº£nh tá»« trong tháº» <img> cá»§a tÃ³m táº¯t
                if summary_raw:
                    soup = BeautifulSoup(summary_raw, 'html.parser')
                    img_tag = soup.find('img')
                    if img_tag and 'src' in img_tag.attrs:
                        image_url = img_tag['src']
                # Láº¥y tÃªn nguá»“n tá»« link bÃ i viáº¿t
                source_name = get_source_name(entry.link)
                # Xá»­ lÃ½ vÃ  kiá»ƒm tra thá»i gian Ä‘Äƒng bÃ i
                if published_time:
                    try:
                        # Giá»¯ nguyÃªn thá»i gian tá»« RSS (Ä‘Ã£ á»Ÿ mÃºi giá» Viá»‡t Nam)
                        parsed_time = parse_date(published_time)
                        
                        # Chuyá»ƒn vá» UTC chá»‰ Ä‘á»ƒ so sÃ¡nh vá»›i time_threshold
                        time_for_comparison = parsed_time.astimezone(timezone.utc)
                        
                        if time_for_comparison >= time_threshold:
                            articles.append({
                                "title": entry.title,
                                "link": entry.link,
                                "summary_raw": summary_raw,
                                "published_time": parsed_time.isoformat(),  # LÆ°u thá»i gian gá»‘c
                                "image_url": image_url,
                                "source": source_name
                            })
                            seen_titles.add(norm_title)
                            seen_links.add(link)
                            entry_count += 1
                    except (ValueError, TypeError):
                        continue
            
            # ThÃªm vÃ o danh sÃ¡ch thÃ nh cÃ´ng
            successful_rss.append(f"{url} - {entry_count} bÃ i viáº¿t")
            print(f"    âœ… ThÃ nh cÃ´ng: {entry_count} bÃ i viáº¿t")
            
        except Exception as e:
            failed_rss.append(f"{url} - Lá»—i: {str(e)}")
            print(f"    âŒ Lá»—i: {str(e)}")
    
    # Hiá»ƒn thá»‹ tá»•ng káº¿t
    print(f"\nğŸ“Š Tá»”NG Káº¾T RSS:")
    print(f"âœ… ThÃ nh cÃ´ng: {len(successful_rss)} RSS")
    print(f"âŒ Tháº¥t báº¡i: {len(failed_rss)} RSS")
    
    if failed_rss:
        print(f"\nâŒ DANH SÃCH RSS THáº¤T Báº I:")
        for failed in failed_rss:
            print(f"  - {failed}")
    
    if successful_rss:
        print(f"\nâœ… DANH SÃCH RSS THÃ€NH CÃ”NG:")
        for success in successful_rss:
            print(f"  - {success}")
    
    print(f"\n-> ÄÃ£ tÃ¬m tháº¥y {len(articles)} bÃ i viáº¿t má»›i (sau khi lá»c trÃ¹ng láº·p).")
    return pd.DataFrame(articles)


def clean_text(df):
    """
    LÃ m sáº¡ch vÃ  xá»­ lÃ½ vÄƒn báº£n tá»« cá»™t 'summary_raw':
    - Chuyá»ƒn chá»¯ thÆ°á»ng, bá» HTML, dáº¥u cÃ¢u.
    - Bá» bÃ i ngáº¯n (<10 tá»«).
    - TÃ¡ch tá»« tiáº¿ng Viá»‡t báº±ng pyvi.
    - Bá» stopword.
    """
    print("\n2. Äang lÃ m sáº¡ch vÄƒn báº£n...")
    

    # B1: LÃ m sáº¡ch vÄƒn báº£n cÆ¡ báº£n
    summary = (df['summary_raw']
               .str.lower()
               .str.replace(r'<.*?>', '', regex=True)
               .str.replace(r'[^\w\s]', '', regex=True))
    df['summary_cleaned'] = summary

    # B2: Bá» NaN, dÃ²ng rá»—ng
    df.dropna(subset=['summary_cleaned'], inplace=True)
    df = df[df['summary_cleaned'].str.strip() != '']

    # B3: Lá»c bÃ i cÃ³ < 10 tá»«
    original_count = len(df)
    df = df[df['summary_cleaned']
                      .str.split()
                      .str.len() >= 10]
    print(f"-> ÄÃ£ lá»c {original_count - len(df)} bÃ i quÃ¡ ngáº¯n.")

    # B4: Tokenize + bá» stopword
    vi_stop = stopwords("vi")
    def remove_stop_pyvi(text):
        tokens = ViTokenizer.tokenize(text).split()
        filtered = [t for t in tokens if t not in vi_stop]
        return " ".join(filtered)

    df['summary_not_stop_word'] = df['summary_cleaned'].apply(remove_stop_pyvi)

    df = df.reset_index(drop=True)
    print("-> HoÃ n táº¥t lÃ m sáº¡ch.")
    return df
def vectorize_text(sentences, model_name):
    """Vector hÃ³a cÃ¢u báº±ng S-BERT."""
    print(f"3/6: Äang vector hÃ³a vÄƒn báº£n báº±ng mÃ´ hÃ¬nh {model_name}...")
    model = SentenceTransformer(model_name)
    embeddings = model.encode(sentences, show_progress_bar=True)
    print("-> Vector hÃ³a hoÃ n táº¥t.")
    return embeddings

def generate_meaningful_topic_name(keywords, sample_titles):
    """Sá»­ dá»¥ng Gemini Ä‘á»ƒ táº¡o tÃªn chá»§ Ä‘á»."""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        prompt = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ biÃªn táº­p bÃ¡o chÃ­. Dá»±a vÃ o cÃ¡c thÃ´ng tin dÆ°á»›i Ä‘Ã¢y, hÃ£y táº¡o ra chá»‰ má»™t tÃªn chá»§ Ä‘á» ngáº¯n gá»n duy nháº¥t (khÃ´ng quÃ¡ 6 tá»«) báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ tÃ³m táº¯t ná»™i dung chÃ­nh.
        CÃ¡c tá»« khÃ³a chÃ­nh cá»§a chá»§ Ä‘á»: {keywords}
        Má»™t vÃ i tiÃªu Ä‘á» bÃ i viáº¿t vÃ­ dá»¥:
        - {"\n- ".join(sample_titles)}
        TÃªn chá»§ Ä‘á» gá»£i Ã½:"""
        response = model.generate_content(prompt)
        return response.text.strip().replace("*", "")
    except Exception as e:
        # In ra lá»—i cá»¥ thá»ƒ hÆ¡n
        print(f"  - Lá»—i khi gá»i Gemini API: {type(e).__name__} - {e}. Sá»­ dá»¥ng tá»« khÃ³a lÃ m nhÃ£n thay tháº¿.")
        return keywords

def get_topic_labels(df, num_keywords=5):
    """GÃ¡n nhÃ£n chá»§ Ä‘á» cho cÃ¡c cá»¥m."""
    print("4/6: Äang gÃ¡n nhÃ£n chá»§ Ä‘á» cho cÃ¡c cá»¥m...")
    topic_labels = {}
    # Sá»­ dá»¥ng sá»‘ cá»¥m thá»±c táº¿ tá»« dá»¯ liá»‡u
    actual_clusters = df['topic_cluster'].nunique()
    for i in range(actual_clusters):
        cluster_df = df[df['topic_cluster'] == i]
        cluster_texts = cluster_df['summary_cleaned'].tolist()
        if len(cluster_texts) < 3:
            topic_labels[str(i)] = "Chá»§ Ä‘á» nhá» (Ã­t bÃ i viáº¿t)"
            continue
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(cluster_texts)
        avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1
        top_indices = avg_tfidf_scores.argsort()[-num_keywords:][::-1]
        feature_names = vectorizer.get_feature_names_out()
        keywords = ", ".join([feature_names[j] for j in top_indices])
        sample_titles = cluster_df['title'].head(3).tolist()
        meaningful_name = generate_meaningful_topic_name(keywords, sample_titles)
        print(f"  - Cluster {i}: {keywords}  =>  TÃªn chá»§ Ä‘á»: {meaningful_name}")
        topic_labels[str(i)] = meaningful_name
        time.sleep(4.1)
    print("-> GÃ¡n nhÃ£n chá»§ Ä‘á» hoÃ n táº¥t.")
    return topic_labels

def find_optimal_clusters(embeddings, max_clusters=20):
    """TÃ¬m sá»‘ cá»¥m (K) tá»‘i Æ°u báº±ng há»‡ sá»‘ Silhouette.
    
    Args:
        embeddings (numpy.ndarray): Ma tráº­n embeddings cá»§a cÃ¡c bÃ i viáº¿t
        max_clusters (int): Sá»‘ cá»¥m tá»‘i Ä‘a cáº§n xem xÃ©t (máº·c Ä‘á»‹nh lÃ  20)
        
    Returns:
        int: Sá»‘ cá»¥m tá»‘i Æ°u (K) cÃ³ há»‡ sá»‘ Silhouette cao nháº¥t
    """
    print("\n3. Äang tÃ¬m sá»‘ cá»¥m (K) tá»‘i Æ°u báº±ng há»‡ sá»‘ Silhouette...")
    
    silhouette_scores = []
    possible_k_values = range(2, max_clusters + 1)

    for k in possible_k_values:
        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans_temp.fit_predict(embeddings)
        score = silhouette_score(embeddings, labels)
        silhouette_scores.append(score)

    # Tá»± Ä‘á»™ng tÃ¬m K cÃ³ silhouette score cao nháº¥t
    best_k_index = np.argmax(silhouette_scores)
    optimal_k = possible_k_values[best_k_index]

    print(f"-> ÄÃ£ tÃ¬m tháº¥y sá»‘ cá»¥m tá»‘i Æ°u lÃ  K={optimal_k}")
    return optimal_k

def main_pipeline():
    """HÃ m chÃ­nh cháº¡y toÃ n bá»™ quy trÃ¬nh."""
    print("\nğŸš€ Báº®T Äáº¦U QUY TRÃŒNH Tá»° Äá»˜NG HÃ“A")
    
    # CÃ¡c bÆ°á»›c giá»¯ nguyÃªn
    df = fetch_recent_articles(RSS_URLS,hours=24)
    if df.empty:
        print("KhÃ´ng cÃ³ bÃ i viáº¿t má»›i nÃ o. Dá»«ng quy trÃ¬nh.")
        return
    df = clean_text(df)
    embeddings = vectorize_text(df['summary_not_stop_word'].tolist(), SBERT_MODEL)
    
    # Thay tháº¿ Ä‘oáº¡n code cÅ© báº±ng lá»i gá»i hÃ m má»›i
    NUM_CLUSTERS = find_optimal_clusters(embeddings)
    # -----------------------------------------------------------------------------


    # BÆ¯á»šC 4: Cháº¡y K-Means cuá»‘i cÃ¹ng vá»›i sá»‘ cá»¥m Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c
    # -----------------------------------------------------------------------------
    print(f"\n4. Äang thá»±c hiá»‡n phÃ¢n cá»¥m K-Means vá»›i {NUM_CLUSTERS} cá»¥m...")
    # Khá»Ÿi táº¡o mÃ´ hÃ¬nh K-Means vá»›i NUM_CLUSTERS Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh
    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)

    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  dá»± Ä‘oÃ¡n cá»¥m
    df['topic_cluster'] = kmeans.fit_predict(embeddings)
    print("-> PhÃ¢n cá»¥m hoÃ n táº¥t.")



    print("\nğŸ“Š PhÃ¢n bá»• sá»‘ lÆ°á»£ng bÃ i viáº¿t vÃ o cÃ¡c cá»¥m (chá»§ Ä‘á»):")
    cluster_counts = df['topic_cluster'].value_counts().sort_index()
    print(cluster_counts)
        
    topic_labels = get_topic_labels(df)
        
    print("5/6: Äang tÃ­nh toÃ¡n ma tráº­n tÆ°Æ¡ng Ä‘á»“ng...")
    cosine_sim = cosine_similarity(embeddings)
    
    print("6/6: Äang lÆ°u cÃ¡c file káº¿t quáº£...")
    df.to_csv('final_articles_for_app.csv', index=False, encoding='utf-8-sig')
    np.save('cosine_similarity_matrix.npy', cosine_sim)
    with open('topic_labels.json', 'w', encoding='utf-8') as f:
        json.dump(topic_labels, f, ensure_ascii=False, indent=4)
    # LÆ°u embeddings.npy cho app
    np.save('embeddings.npy', embeddings)
    print("âœ… ÄÃ£ lÆ°u embeddings.npy.")
    print("\nâœ… QUY TRÃŒNH HOÃ€N Táº¤T! âœ…")

if __name__ == "__main__":
    main_pipeline()
